{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* [Intro](#Intro)\n",
    "* [Imports and config](#Imports-and-config)\n",
    "* [Load data](#Load-data)\n",
    "* [Train test split](#Train-test-split)\n",
    "* [Convolutional Neural Network](#Convolutional-Neural-Network)\n",
    "  * [Ternary](#Ternary)\n",
    "    * [Fit ternary](#Fit-ternary)\n",
    "    * [Results ternary](#Results-ternary)\n",
    "  * [Binary](#Binary)\n",
    "      * [Fit binary](#Fit-binary)\n",
    "      * [Results binary](#Results-binary)\n",
    "* [Discussion](#Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook sets up a Convolutional Neural Network (CNN) to classify audio by spectrogram input. Both ternary and binary classification are considered. In all cases except for the binary positive/non-positive case, the trained classifier was able to outperform the dummy classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(SEED := 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extensions\n",
    "%load_ext lab_black\n",
    "%load_ext nb_black\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.88 s\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    GlobalMaxPooling2D,\n",
    "    Dense,\n",
    ")\n",
    "import tensorflow as tf\n",
    "\n",
    "# display outputs w/o print calls\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.98 ms\n"
     ]
    }
   ],
   "source": [
    "# Location of parquet\n",
    "PARQUET_DF_FOLDER = \"../5.0-mic-extract_spectrograms_and_MFCCs_short\"\n",
    "\n",
    "# Location where this notebook will output\n",
    "DATA_OUT_FOLDER = \".\"\n",
    "\n",
    "# The preprocessed data from the Unified Multilingual Dataset of Emotional Human utterances\n",
    "WAV_DIRECTORY = (\n",
    "    \"../../unified_multilingual_dataset_of_emotional_human_utterances/data/preprocessed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>duration</th>\n",
       "      <th>source</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>speaker_gender</th>\n",
       "      <th>emo</th>\n",
       "      <th>valence</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang2</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>length</th>\n",
       "      <th>padded</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>melspec_db</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01788+BAUM1+BAUM1.s028+f+hap+1+tur+tr-tr.wav</td>\n",
       "      <td>0.387</td>\n",
       "      <td>BAUM1</td>\n",
       "      <td>BAUM1.s028</td>\n",
       "      <td>f</td>\n",
       "      <td>hap</td>\n",
       "      <td>1</td>\n",
       "      <td>tur</td>\n",
       "      <td>tr-tr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>short</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[-680.11646, -680.11646, -673.7514, -377.4224...</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file duration source  speaker_id  \\\n",
       "0  01788+BAUM1+BAUM1.s028+f+hap+1+tur+tr-tr.wav    0.387  BAUM1  BAUM1.s028   \n",
       "\n",
       "  speaker_gender  emo valence lang1  lang2  neg  neu  pos length  \\\n",
       "0              f  hap       1   tur  tr-tr    0    0    1  short   \n",
       "\n",
       "                                              padded  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                mfcc  \\\n",
       "0  [[-680.11646, -680.11646, -673.7514, -377.4224...   \n",
       "\n",
       "                                          melspec_db  \n",
       "0  [[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -7...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "short_df = pd.read_parquet(f\"{PARQUET_DF_FOLDER}/short_plus.parquet\")\n",
    "short_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom split ensures no data leakage due to speaker characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "short_speakers = (\n",
    "    pd.DataFrame(np.unique(short_df.speaker_id))\n",
    "    .sample(frac=0.30, random_state=SEED)[0]\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 in test, 290 in train\n",
      "time: 22.3 ms\n"
     ]
    }
   ],
   "source": [
    "X_test = (_ := short_df.loc[short_df.speaker_id.isin(short_speakers)]).melspec_db\n",
    "y_test = _.valence\n",
    "X_train = (_ := short_df.loc[~short_df.speaker_id.isin(short_speakers)]).melspec_db\n",
    "y_train = _.valence\n",
    "len(short_df) == len(y_test) + len(y_train)\n",
    "print(f\"{len(y_test)} in test, {len(y_train)} in train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional preprocessing is needed to format the data for keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 86.6 ms\n"
     ]
    }
   ],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3, dtype=\"float32\")\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3, dtype=\"float32\")\n",
    "\n",
    "stack = np.stack\n",
    "reshaper: np.ndarray = lambda x: stack(x.apply(lambda _: stack(_))).reshape(\n",
    "    len(x), 128, 16, 1\n",
    ")\n",
    "\n",
    "X_train, X_test = reshaper(X_train), reshaper(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a simple CNN. After the input layer, there is one convolutional layer, a global max pooling layer, and a softmax output layer for three classes. I chose global max pooling over the local analogue since spectral features are not localized to a single part of the image (unlike the edges of a shape in object detection, for instance). I did a bit of trial and error in configuring the architecture (not documented here) and it seemed to work better this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 14, 128)      1280      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,667\n",
      "Trainable params: 1,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 652 ms\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential(\n",
    "    [\n",
    "        Conv2D(filters=128, kernel_size=3, activation=\"relu\", input_shape=(128, 16, 1)),\n",
    "        GlobalMaxPooling2D(),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the ternary classifier works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "290/290 [==============================] - 3s 7ms/step - loss: 2.5130 - accuracy: 0.3552 - val_loss: 1.2435 - val_accuracy: 0.4579\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 2.2186 - accuracy: 0.3724 - val_loss: 1.8867 - val_accuracy: 0.2263\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 1.7308 - accuracy: 0.4069 - val_loss: 3.5172 - val_accuracy: 0.2053\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 1.4775 - accuracy: 0.4414 - val_loss: 1.5696 - val_accuracy: 0.3632\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 2s 8ms/step - loss: 1.5478 - accuracy: 0.4310 - val_loss: 1.2129 - val_accuracy: 0.4526\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 2s 8ms/step - loss: 1.5024 - accuracy: 0.4586 - val_loss: 1.6055 - val_accuracy: 0.4526\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 1.2316 - accuracy: 0.4828 - val_loss: 3.2073 - val_accuracy: 0.2053\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 1.4031 - accuracy: 0.4966 - val_loss: 1.2390 - val_accuracy: 0.5474\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 2s 8ms/step - loss: 1.1663 - accuracy: 0.5276 - val_loss: 1.5216 - val_accuracy: 0.4842\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 3s 9ms/step - loss: 1.1631 - accuracy: 0.5138 - val_loss: 1.0792 - val_accuracy: 0.4737\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 3s 9ms/step - loss: 1.2326 - accuracy: 0.4966 - val_loss: 1.7566 - val_accuracy: 0.3526\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 2s 8ms/step - loss: 1.1513 - accuracy: 0.4724 - val_loss: 1.6572 - val_accuracy: 0.2684\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 2s 8ms/step - loss: 1.1333 - accuracy: 0.5034 - val_loss: 1.0728 - val_accuracy: 0.4737\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 2s 8ms/step - loss: 1.0342 - accuracy: 0.5414 - val_loss: 1.1179 - val_accuracy: 0.4684\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 2s 8ms/step - loss: 1.0902 - accuracy: 0.5138 - val_loss: 1.2290 - val_accuracy: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x232995d3bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "model_cnn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well would a dummy classifier do in ternary classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy classifer:\n",
      "85 samples of valence 0 in test split (0.447 / 0.553)\n",
      "66 samples of valence -1 in test split (0.347 / 0.653)\n",
      "39 samples of valence 1 in test split (0.205 / 0.795)\n",
      "time: 24.8 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"dummy classifer:\")\n",
    "len_full_test = len(X_test)\n",
    "for valence in {\"-1\", \"0\", \"1\"}:\n",
    "    test_valence_set = short_df.loc[\n",
    "        (short_df.valence == valence) & short_df.speaker_id.isin(short_speakers)\n",
    "    ]\n",
    "    print(\n",
    "        f\"{(_ := len(test_valence_set.loc[test_valence_set.valence == valence]))} samples of valence {valence} in test split ({(__ := _ / len_full_test):.3f} / {1 - __:.3f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation accuracy from the fifteen epochs above was about 54.7%. It outperformed the dummy classifier (best score of 44.7%) by about 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up to run the same architecture with slight modifications for the binary cases. Namely, the output layer reflects the number of classes and uses a sigmoid activation function rather than softmax; also, the loss function was changed from categorical cross entropy to binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "criterion = short_df.speaker_id.isin(short_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.73 ms\n"
     ]
    }
   ],
   "source": [
    "OvrSet = namedtuple(\"OvrSet\", \"name, test, train, dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.9 ms\n"
     ]
    }
   ],
   "source": [
    "binary_valence = [\n",
    "    OvrSet(\n",
    "        name=valence,\n",
    "        test=(_ := short_df.loc[criterion][valence]),\n",
    "        train=short_df.loc[~criterion][valence],\n",
    "        dummy=_.apply(lambda _: _ == 1).sum() / len(_),\n",
    "    )\n",
    "    for valence in (\"neg\", \"neu\", \"pos\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loops through the binary classification sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valence: neg dummy score: 0.6526315789473685\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 14, 128)      1280      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,409\n",
      "Trainable params: 1,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "290/290 [==============================] - 3s 7ms/step - loss: 1.3211 - accuracy: 0.5000 - val_loss: 1.0554 - val_accuracy: 0.3842\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 1.0440 - accuracy: 0.6103 - val_loss: 1.0849 - val_accuracy: 0.6526\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 1.2426 - accuracy: 0.5862 - val_loss: 1.5927 - val_accuracy: 0.6526\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.8762 - accuracy: 0.6069 - val_loss: 0.6535 - val_accuracy: 0.6211\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.7696 - accuracy: 0.6310 - val_loss: 1.3312 - val_accuracy: 0.6526\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6534 - accuracy: 0.6552 - val_loss: 0.9471 - val_accuracy: 0.4263\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.7800 - accuracy: 0.6448 - val_loss: 1.0319 - val_accuracy: 0.3842\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.7046 - accuracy: 0.6345 - val_loss: 0.9295 - val_accuracy: 0.6526\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6538 - accuracy: 0.6517 - val_loss: 0.6329 - val_accuracy: 0.6737\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6445 - accuracy: 0.6759 - val_loss: 0.6517 - val_accuracy: 0.6684\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.7311 - accuracy: 0.6172 - val_loss: 0.7345 - val_accuracy: 0.6526\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6420 - accuracy: 0.6690 - val_loss: 0.7823 - val_accuracy: 0.6526\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6399 - accuracy: 0.6690 - val_loss: 0.6336 - val_accuracy: 0.6842\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6351 - accuracy: 0.6690 - val_loss: 0.7387 - val_accuracy: 0.5421\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6713 - accuracy: 0.6517 - val_loss: 0.7587 - val_accuracy: 0.5474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2329c2e5c40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valence: neu dummy score: 0.5526315789473684\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 126, 14, 128)      1280      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,409\n",
      "Trainable params: 1,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 1.1478 - accuracy: 0.6414 - val_loss: 0.7124 - val_accuracy: 0.5737\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 1.3513 - accuracy: 0.6103 - val_loss: 0.7117 - val_accuracy: 0.5789\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 2s 5ms/step - loss: 1.0020 - accuracy: 0.6310 - val_loss: 0.8464 - val_accuracy: 0.6053\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.7538 - accuracy: 0.6552 - val_loss: 0.6604 - val_accuracy: 0.6474\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.8857 - accuracy: 0.6655 - val_loss: 2.0370 - val_accuracy: 0.5526\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.9719 - accuracy: 0.6103 - val_loss: 0.7123 - val_accuracy: 0.6053\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.7329 - accuracy: 0.6655 - val_loss: 0.8817 - val_accuracy: 0.5842\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6275 - accuracy: 0.6483 - val_loss: 1.0985 - val_accuracy: 0.5526\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6050 - accuracy: 0.7207 - val_loss: 0.9070 - val_accuracy: 0.5526\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.5894 - accuracy: 0.7103 - val_loss: 0.6409 - val_accuracy: 0.6211\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6263 - accuracy: 0.6759 - val_loss: 0.8068 - val_accuracy: 0.6105\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.5696 - accuracy: 0.7069 - val_loss: 1.9060 - val_accuracy: 0.5526\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.5830 - accuracy: 0.7379 - val_loss: 0.6930 - val_accuracy: 0.5947\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.5213 - accuracy: 0.7552 - val_loss: 0.6310 - val_accuracy: 0.6526\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.5446 - accuracy: 0.7207 - val_loss: 0.9230 - val_accuracy: 0.5579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2329c6911c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valence: pos dummy score: 0.7947368421052632\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 126, 14, 128)      1280      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,409\n",
      "Trainable params: 1,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 1.3880 - accuracy: 0.5966 - val_loss: 0.8479 - val_accuracy: 0.7947\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 1.1179 - accuracy: 0.5897 - val_loss: 1.0995 - val_accuracy: 0.7947\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 0.9559 - accuracy: 0.6034 - val_loss: 0.8524 - val_accuracy: 0.7947\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 0.8388 - accuracy: 0.6586 - val_loss: 0.5768 - val_accuracy: 0.6947\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 0.7109 - accuracy: 0.6414 - val_loss: 0.5909 - val_accuracy: 0.7105\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 0.7668 - accuracy: 0.6621 - val_loss: 1.0432 - val_accuracy: 0.2421\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 0.8779 - accuracy: 0.6172 - val_loss: 0.8537 - val_accuracy: 0.4053\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 1s 5ms/step - loss: 0.6839 - accuracy: 0.6724 - val_loss: 0.9640 - val_accuracy: 0.3158\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 2s 6ms/step - loss: 0.6996 - accuracy: 0.6828 - val_loss: 0.6586 - val_accuracy: 0.7947\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6872 - accuracy: 0.6552 - val_loss: 0.8443 - val_accuracy: 0.3789\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6131 - accuracy: 0.7069 - val_loss: 0.5639 - val_accuracy: 0.7474\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6718 - accuracy: 0.6621 - val_loss: 0.5364 - val_accuracy: 0.7895\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.7132 - accuracy: 0.6862 - val_loss: 0.5985 - val_accuracy: 0.7947\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.6043 - accuracy: 0.7103 - val_loss: 0.7125 - val_accuracy: 0.7947\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 2s 7ms/step - loss: 0.5496 - accuracy: 0.7241 - val_loss: 0.5333 - val_accuracy: 0.7684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2329dad5b50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "for ovr_set in binary_valence:\n",
    "    dummy = ovr_set.dummy\n",
    "    print(\"valence:\", ovr_set.name, \"dummy score:\", dummy if dummy > 0.5 else 1 - dummy)\n",
    "    model_cnn = Sequential(\n",
    "        [\n",
    "            Conv2D(\n",
    "                filters=128, kernel_size=3, activation=\"relu\", input_shape=(128, 16, 1)\n",
    "            ),\n",
    "            GlobalMaxPooling2D(),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_cnn.compile(\n",
    "        loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model_cnn.summary()\n",
    "    model_cnn.fit(\n",
    "        X_train,\n",
    "        ovr_set.train,\n",
    "        validation_data=(X_test, ovr_set.test),\n",
    "        epochs=15,\n",
    "        batch_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the negative/non-negative case, the dummy score on the test set was 65.3%, which underperformed the CNN classifier's best validation score of 68.4% by about 2.1%.\n",
    "\n",
    "In the neutral/non-neutral case, the dummy score on the test set was 55.3%, which underperformed the CNN classifier's best validation score of 65.3% by about 10.0%.\n",
    "\n",
    "In the positive/non-positive case, the dummy score on the test set was 79.5%, which was approximately equal to the classifier's best validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing model in the ternary case as assessed by the best validation score of 15 epochs noticeably outperformed the dummy classifier, but the score did not surpass 55%.\n",
    "\n",
    "In the binary cases, only the positive/non-positive classifier failed to surpass the dummy classifier's performance of the dummy classifier. This is the only case where the accuracy surpassed 70%. Class imbalance may be a factor.\n",
    "\n",
    "It may be better for the three one-vs-rest classifiers to share their lower layers; ensembling these would likely yield better performance.\n",
    "\n",
    "Overall, the results are unremarkable, but there are many possible improvements to be considered. Firstly, we only have a few hundred observations in our subsample. Secondly, the architecture could be reconfigured with more layers and nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5ec24c6db6b01668153f0d7970357104795ba99b94731b011ffeae27cd547f5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
