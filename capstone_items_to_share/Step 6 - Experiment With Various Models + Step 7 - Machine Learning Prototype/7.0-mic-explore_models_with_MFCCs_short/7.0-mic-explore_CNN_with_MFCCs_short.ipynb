{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* [Intro](#Intro)\n",
    "* [Imports and config](#Imports-and-config)\n",
    "* [Load data](#Load-data)\n",
    "* [Train test split](#Train-test-split)\n",
    "* [Convolutional Neural Network](#Convolutional-Neural-Network)\n",
    "  * [Ternary](#Ternary)\n",
    "    * [Fit ternary](#Fit-ternary)\n",
    "    * [Results ternary](#Results-ternary)\n",
    "  * [Binary](#Binary)\n",
    "      * [Fit binary](#Fit-binary)\n",
    "      * [Results binary](#Results-binary)\n",
    "* [Discussion](#Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook sets up a Convolutional Neural Network (CNN) to classify audio of short duration by extracted Mel Frequencey Cepstral Coefficients (MFCCs). Both ternary and binary classification are considered. In all cases except for the binary positive/non-positive case, the trained classifier was able to outperform the dummy classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(SEED := 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extensions\n",
    "%load_ext lab_black\n",
    "%load_ext nb_black\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import swifter\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    GlobalMaxPooling2D,\n",
    "    Dense,\n",
    ")\n",
    "import tensorflow as tf\n",
    "\n",
    "# display outputs w/o print calls\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "# Location of parquet\n",
    "PARQUET_DF_FOLDER = \"../5.0-mic-extract_spectrograms_and_MFCCs_short\"\n",
    "\n",
    "# Location where this notebook will output\n",
    "DATA_OUT_FOLDER = \".\"\n",
    "\n",
    "# The preprocessed data from the Unified Multilingual Dataset of Emotional Human utterances\n",
    "WAV_DIRECTORY = (\n",
    "    \"../../../unified_multilingual_dataset_of_emotional_human_utterances/data/preprocessed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>duration</th>\n",
       "      <th>source</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>speaker_gender</th>\n",
       "      <th>emo</th>\n",
       "      <th>valence</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang2</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>length</th>\n",
       "      <th>padded</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>melspec_db</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01788+BAUM1+BAUM1.s028+f+hap+1+tur+tr-tr.wav</td>\n",
       "      <td>0.387</td>\n",
       "      <td>BAUM1</td>\n",
       "      <td>BAUM1.s028</td>\n",
       "      <td>f</td>\n",
       "      <td>hap</td>\n",
       "      <td>1</td>\n",
       "      <td>tur</td>\n",
       "      <td>tr-tr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>short</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[-680.11646, -680.11646, -673.7514, -377.4224...</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file duration source  speaker_id  \\\n",
       "0  01788+BAUM1+BAUM1.s028+f+hap+1+tur+tr-tr.wav    0.387  BAUM1  BAUM1.s028   \n",
       "\n",
       "  speaker_gender  emo valence lang1  lang2  neg  neu  pos length  \\\n",
       "0              f  hap       1   tur  tr-tr    0    0    1  short   \n",
       "\n",
       "                                              padded  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                mfcc  \\\n",
       "0  [[-680.11646, -680.11646, -673.7514, -377.4224...   \n",
       "\n",
       "                                          melspec_db  \n",
       "0  [[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -7...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "short_df = pd.read_parquet(f\"{PARQUET_DF_FOLDER}/short_plus.parquet\")\n",
    "short_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom split ensures no data leakage due to speaker characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9 ms\n"
     ]
    }
   ],
   "source": [
    "short_speakers = (\n",
    "    pd.DataFrame(np.unique(short_df.speaker_id))\n",
    "    .sample(frac=0.30, random_state=SEED)[0]\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 in test, 290 in train\n",
      "time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "criterion = short_df.speaker_id.isin(short_speakers)\n",
    "X_test = (_ := short_df.loc[criterion]).mfcc\n",
    "y_test = _.valence\n",
    "X_train = (_ := short_df.loc[~criterion]).mfcc\n",
    "y_train = _.valence\n",
    "len(short_df) == len(y_test) + len(y_train)\n",
    "print(f\"{len(y_test)} in test, {len(y_train)} in train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional preprocessing is needed to format the data for keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 290/290 [00:00<00:00, 22331.02it/s]\n",
      "Pandas Apply: 100%|██████████| 190/190 [00:00<00:00, 18997.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3, dtype=\"float32\")\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3, dtype=\"float32\")\n",
    "\n",
    "stack = np.stack\n",
    "reshaper: np.ndarray = lambda x: stack(x.swifter.apply(stack)).reshape(\n",
    "    len(x), 16, 20, 1\n",
    ")\n",
    "X_train, X_test = reshaper(X_train), reshaper(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a simple CNN. After the input layer, there is one convolutional layer, a global max pooling layer, and a softmax output layer for three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 15, 19, 64)        320       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 515\n",
      "Trainable params: 515\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 621 ms\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential(\n",
    "    [\n",
    "        Conv2D(filters=64, kernel_size=2, activation=\"relu\", input_shape=(16, 20, 1)),\n",
    "        GlobalMaxPooling2D(),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the ternary classifier works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 9.9274 - accuracy: 0.4000 - val_loss: 4.3067 - val_accuracy: 0.3105\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 4.8545 - accuracy: 0.4241 - val_loss: 3.6765 - val_accuracy: 0.3632\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 4.4953 - accuracy: 0.4862 - val_loss: 2.9881 - val_accuracy: 0.3684\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 3.1725 - accuracy: 0.4828 - val_loss: 6.5195 - val_accuracy: 0.1947\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 3.3266 - accuracy: 0.5138 - val_loss: 3.6179 - val_accuracy: 0.3895\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.7131 - accuracy: 0.5103 - val_loss: 3.5614 - val_accuracy: 0.4421\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 3.0475 - accuracy: 0.4517 - val_loss: 2.8889 - val_accuracy: 0.3316\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 2.6141 - accuracy: 0.5138 - val_loss: 2.5596 - val_accuracy: 0.4421\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.9600 - accuracy: 0.4483 - val_loss: 4.2156 - val_accuracy: 0.4368\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 3.0447 - accuracy: 0.4862 - val_loss: 3.6190 - val_accuracy: 0.5526\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.5092 - accuracy: 0.5103 - val_loss: 2.9745 - val_accuracy: 0.3789\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 2.3975 - accuracy: 0.4966 - val_loss: 2.4882 - val_accuracy: 0.4421\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.6526 - accuracy: 0.4862 - val_loss: 4.2810 - val_accuracy: 0.2947\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 2.0571 - accuracy: 0.5897 - val_loss: 3.9586 - val_accuracy: 0.3947\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.3676 - accuracy: 0.5103 - val_loss: 4.1011 - val_accuracy: 0.3789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x166c5d92bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.43 s\n"
     ]
    }
   ],
   "source": [
    "model_cnn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well would a dummy classifier do in ternary classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy classifer:\n",
      "39 samples of valence 1 in test split (0.205 / 0.795)\n",
      "66 samples of valence -1 in test split (0.347 / 0.653)\n",
      "85 samples of valence 0 in test split (0.447 / 0.553)\n",
      "time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"dummy classifer:\")\n",
    "len_full_test = len(X_test)\n",
    "for valence in {\"-1\", \"0\", \"1\"}:\n",
    "    test_valence_set = short_df.loc[\n",
    "        (short_df.valence == valence) & short_df.speaker_id.isin(short_speakers)\n",
    "    ]\n",
    "    print(\n",
    "        f\"{(_ := len(test_valence_set.loc[test_valence_set.valence == valence]))} samples of valence {valence} in test split ({(__ := _ / len_full_test):.3f} / {1 - __:.3f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation accuracy from the fifteen epochs above was about 55.3%. It outperformed the dummy classifier (best score of 44.7%) by about 10.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up to run the same architecture with slight modifications for the binary cases. Namely, the output layer reflects the number of classes and uses a sigmoid activation function rather than softmax; also, the loss function was changed from categorical cross entropy to binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "OvrSet = namedtuple(\"OvrSet\", \"name, test, train, dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21 ms\n"
     ]
    }
   ],
   "source": [
    "binary_valence = [\n",
    "    OvrSet(\n",
    "        name=valence,\n",
    "        test=(_ := short_df.loc[criterion][valence]),\n",
    "        train=short_df.loc[~criterion][valence],\n",
    "        dummy=_.swifter.apply(lambda _: _ == 1).sum() / len(_),\n",
    "    )\n",
    "    for valence in (\"neg\", \"neu\", \"pos\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loops through the binary classification sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valence: neg dummy score: 0.6526315789473685\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 15, 19, 64)        320       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 4.5500 - accuracy: 0.5828 - val_loss: 2.5321 - val_accuracy: 0.6895\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 3.9348 - accuracy: 0.5759 - val_loss: 2.4665 - val_accuracy: 0.6842\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 1.9232 - accuracy: 0.6241 - val_loss: 1.4409 - val_accuracy: 0.5684\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.0540 - accuracy: 0.6414 - val_loss: 1.1798 - val_accuracy: 0.6000\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 2.2129 - accuracy: 0.6345 - val_loss: 6.7951 - val_accuracy: 0.3684\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.4920 - accuracy: 0.5931 - val_loss: 2.4352 - val_accuracy: 0.6526\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.8710 - accuracy: 0.6690 - val_loss: 3.1445 - val_accuracy: 0.4211\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 1.9910 - accuracy: 0.6103 - val_loss: 1.2967 - val_accuracy: 0.6368\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.0726 - accuracy: 0.6621 - val_loss: 2.1721 - val_accuracy: 0.6684\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.3418 - accuracy: 0.6517 - val_loss: 2.7261 - val_accuracy: 0.6579\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.8753 - accuracy: 0.6310 - val_loss: 1.4649 - val_accuracy: 0.5316\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.4131 - accuracy: 0.6621 - val_loss: 2.2570 - val_accuracy: 0.6684\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.3441 - accuracy: 0.6897 - val_loss: 1.7769 - val_accuracy: 0.6632\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.7669 - accuracy: 0.6276 - val_loss: 1.3307 - val_accuracy: 0.6316\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.5662 - accuracy: 0.6828 - val_loss: 2.6810 - val_accuracy: 0.6632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x166c97bde20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valence: neu dummy score: 0.5526315789473684\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 15, 19, 64)        320       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 4.0531 - accuracy: 0.5724 - val_loss: 2.5015 - val_accuracy: 0.5789\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.9142 - accuracy: 0.6552 - val_loss: 1.4641 - val_accuracy: 0.6158\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 2.4292 - accuracy: 0.6448 - val_loss: 1.8618 - val_accuracy: 0.6000\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.7918 - accuracy: 0.6621 - val_loss: 2.5286 - val_accuracy: 0.6053\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 1.6779 - accuracy: 0.6828 - val_loss: 2.7597 - val_accuracy: 0.5789\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.4304 - accuracy: 0.6931 - val_loss: 1.3248 - val_accuracy: 0.6316\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.3892 - accuracy: 0.6931 - val_loss: 1.4424 - val_accuracy: 0.6000\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.1537 - accuracy: 0.7207 - val_loss: 1.0000 - val_accuracy: 0.6316\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.7284 - accuracy: 0.6793 - val_loss: 2.0636 - val_accuracy: 0.5684\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.3919 - accuracy: 0.6862 - val_loss: 1.0651 - val_accuracy: 0.6368\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 1.7484 - accuracy: 0.6759 - val_loss: 1.9321 - val_accuracy: 0.5789\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.2753 - accuracy: 0.7345 - val_loss: 1.1048 - val_accuracy: 0.6632\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 1.1336 - accuracy: 0.7103 - val_loss: 2.5448 - val_accuracy: 0.5632\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.1475 - accuracy: 0.7034 - val_loss: 1.0723 - val_accuracy: 0.6368\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 1.2809 - accuracy: 0.6966 - val_loss: 1.6002 - val_accuracy: 0.6158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x166cabfd790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valence: pos dummy score: 0.7947368421052632\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 15, 19, 64)        320       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 5.9032 - accuracy: 0.5966 - val_loss: 1.5506 - val_accuracy: 0.7737\n",
      "Epoch 2/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.4259 - accuracy: 0.6586 - val_loss: 0.9686 - val_accuracy: 0.6105\n",
      "Epoch 3/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.8299 - accuracy: 0.6241 - val_loss: 1.5476 - val_accuracy: 0.7842\n",
      "Epoch 4/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.3367 - accuracy: 0.5862 - val_loss: 1.2672 - val_accuracy: 0.7211\n",
      "Epoch 5/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.4302 - accuracy: 0.6138 - val_loss: 1.8366 - val_accuracy: 0.4737\n",
      "Epoch 6/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 1.7759 - accuracy: 0.6241 - val_loss: 2.5020 - val_accuracy: 0.3526\n",
      "Epoch 7/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 2.0233 - accuracy: 0.6655 - val_loss: 1.6828 - val_accuracy: 0.4947\n",
      "Epoch 8/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.2426 - accuracy: 0.6897 - val_loss: 1.1991 - val_accuracy: 0.5947\n",
      "Epoch 9/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.9291 - accuracy: 0.6448 - val_loss: 3.2807 - val_accuracy: 0.7947\n",
      "Epoch 10/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.2538 - accuracy: 0.6655 - val_loss: 1.0454 - val_accuracy: 0.5947\n",
      "Epoch 11/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.1853 - accuracy: 0.6414 - val_loss: 2.2324 - val_accuracy: 0.3684\n",
      "Epoch 12/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.9415 - accuracy: 0.6276 - val_loss: 1.2227 - val_accuracy: 0.7053\n",
      "Epoch 13/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 1.6900 - accuracy: 0.6966 - val_loss: 4.0034 - val_accuracy: 0.7947\n",
      "Epoch 14/15\n",
      "290/290 [==============================] - 1s 2ms/step - loss: 2.0757 - accuracy: 0.6517 - val_loss: 3.6637 - val_accuracy: 0.7947\n",
      "Epoch 15/15\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 2.1325 - accuracy: 0.6310 - val_loss: 6.4429 - val_accuracy: 0.2474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x166caf30e20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "for ovr_set in binary_valence:\n",
    "    dummy = ovr_set.dummy\n",
    "    print(\"valence:\", ovr_set.name, \"dummy score:\", dummy if dummy > 0.5 else 1 - dummy)\n",
    "    model_cnn = Sequential(\n",
    "        [\n",
    "            Conv2D(\n",
    "                filters=64, kernel_size=2, activation=\"relu\", input_shape=(16, 20, 1)\n",
    "            ),\n",
    "            GlobalMaxPooling2D(),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_cnn.compile(\n",
    "        loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model_cnn.summary()\n",
    "    model_cnn.fit(\n",
    "        X_train,\n",
    "        ovr_set.train,\n",
    "        validation_data=(X_test, ovr_set.test),\n",
    "        epochs=15,\n",
    "        batch_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the negative/non-negative case, the dummy score on the test set was 65.3%, which underperformed the CNN classifier's best validation score of 69.0% by about 3.7%.\n",
    "\n",
    "In the neutral/non-neutral case, the dummy score on the test set was 55.3%, which underperformed the CNN classifier's best validation score of 66.3% by about 11.1%.\n",
    "\n",
    "In the positive/non-positive case, the dummy score on the test set was 79.5%, which was approximately equal to the classifier's best validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing model in the ternary case as assessed by the best validation score of 15 epochs noticeably outperformed the dummy classifier, but the score just barely surpassed 55%.\n",
    "\n",
    "In the binary cases, only the positive/non-positive classifier failed to surpass the dummy classifier's performance of the dummy classifier. This is the only case where the accuracy surpassed 70%. Class imbalance may play a role.\n",
    "\n",
    "It may be better for the three one-vs-rest classifiers to share their lower layers; ensembling these would likely yield better performance.\n",
    "\n",
    "Overall, the results are unremarkable, but there are many possible improvements to be considered. Firstly, we only have a few hundred observations in our subsample. Secondly, the architecture could be reconfigured with more layers and nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5ec24c6db6b01668153f0d7970357104795ba99b94731b011ffeae27cd547f5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
