{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Imports and configuration](#Imports-and-configuration)\n",
    "* [Load data](#Load-data)\n",
    "* [Setup](#Setup)\n",
    "* [Evaluate](#Evaluate)\n",
    "  * [By speaker gender](#By-speaker-gender)\n",
    "  * [By language](#By-language)\n",
    "  * [By valence (OvR)](#By-valence-(OvR))\n",
    "  * [One English sample](#One-English-sample)\n",
    "* [Results](#Results)\n",
    "* [Discussion](#Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We have now trained 4 prototypes on features engineered from FRILL embeddings. In addition, we have extracted the same features from holdout data from three new sources. In this notebook, we evaluate the classification performance of the prototypes on the unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "notebook_begin_time = time()\n",
    "\n",
    "# set random seeds\n",
    "\n",
    "from os import environ\n",
    "from random import seed as random_seed\n",
    "from numpy.random import seed as np_seed\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "\n",
    "def reset_seeds(seed: int) -> None:\n",
    "    \"\"\"Utility function for resetting random seeds\"\"\"\n",
    "    environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random_seed(seed)\n",
    "    np_seed(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "\n",
    "reset_seeds(SEED := 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extensions\n",
    "%load_ext autotime\n",
    "%load_ext lab_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utility\n",
    "from gc import collect as gc_collect\n",
    "from joblib import dump, load\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# typing\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "# faster sklearn\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "del patch_sklearn\n",
    "\n",
    "# metrics\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    brier_score_loss,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    top_k_accuracy_score,\n",
    ")\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "%matplotlib inline\n",
    "\n",
    "# full display of rows and columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# display outputs w/o print calls\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "del InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of holdout data\n",
    "HOLDOUT_DATA = \".\"\n",
    "\n",
    "# Location of holdout labels\n",
    "HOLDOUT_LABELS = \".\"\n",
    "\n",
    "# Location where this notebook will output\n",
    "OUT_FOLDER = \".\"\n",
    "\n",
    "# Location of prototype joblib files\n",
    "PROTOTYPES = \"./prototypes\"\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"{HOLDOUT_DATA}/holdout_featurized.feather\")\n",
    "labels = pd.read_feather(f\"{HOLDOUT_LABELS}/holdout_labels.feather\")\n",
    "labels = labels.loc[labels.emo != \"sur\"]\n",
    "labels = labels.loc[data.index]\n",
    "assert len(labels) == len(data)\n",
    "assert all(labels.index == data.index)\n",
    "\n",
    "y_true = labels.valence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this section, we define some structures to keep track of models and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = (\n",
    "    \"bagging_GNB\",\n",
    "    \"logreg\",\n",
    "    \"ridge\",\n",
    "    \"stacked_gnb_ridge\",\n",
    "    \"stacked_gnb_ridge_passthrough\",\n",
    "    \"voting_gnb_ridge\",\n",
    ")\n",
    "load_model = lambda model: load(f\"{PROTOTYPES}/{model}.joblib\")\n",
    "\n",
    "# ternary to binary labels per class\n",
    "RECODER = {\n",
    "    \"neg\": lambda y: ((y - 1) // 2) * (-1),\n",
    "    \"neu\": lambda y: y % 2,\n",
    "    \"pos\": lambda y: y // 2,\n",
    "}\n",
    "\n",
    "VALENCE = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
    "\n",
    "\n",
    "def binary_scorer(scorer: Callable, valence: str) -> Callable:\n",
    "    \"Returns a binary scorer for the given class (valence)\"\n",
    "\n",
    "    def new_scorer(y_true, y_score) -> float:\n",
    "        y_true = RECODER[valence](y_true)\n",
    "        y_score = RECODER[valence](np.squeeze(y_score))\n",
    "        try:  # for predict_proba output\n",
    "            _ = len(y_score[0])  # len(int) should throw a TypeError\n",
    "            y_score: np.ndarray = y_score[:, VALENCE[valence]]\n",
    "        except TypeError:\n",
    "            y_score = RECODER[valence](y_score)\n",
    "        return scorer(y_true, y_score)\n",
    "\n",
    "    return new_scorer\n",
    "\n",
    "\n",
    "METRICS = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"adjusted_balanced_accuracy\": lambda y_true, y_pred: balanced_accuracy_score(\n",
    "        y_true, y_pred, adjusted=True\n",
    "    ),\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,  # default\n",
    "    \"macro_f1\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    \"micro_f1\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"micro\"),\n",
    "    \"weighted_f1\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "    \"macro_precision\": lambda y_true, y_pred: precision_score(\n",
    "        y_true, y_pred, average=\"macro\"\n",
    "    ),\n",
    "    \"micro_precision\": lambda y_true, y_pred: precision_score(\n",
    "        y_true, y_pred, average=\"micro\"\n",
    "    ),\n",
    "    \"weighted_precision\": lambda y_true, y_pred: precision_score(\n",
    "        y_true, y_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"macro_recall\": lambda y_true, y_pred: recall_score(\n",
    "        y_true, y_pred, average=\"macro\"\n",
    "    ),\n",
    "    \"micro_recall\": lambda y_true, y_pred: recall_score(\n",
    "        y_true, y_pred, average=\"micro\"\n",
    "    ),\n",
    "    \"weighted_recall\": lambda y_true, y_pred: recall_score(\n",
    "        y_true, y_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"geometric_mean\": lambda y_true, y_pred: geometric_mean_score(\n",
    "        y_true, y_pred, average=\"multiclass\"\n",
    "    ),\n",
    "    \"macro_geometric_mean\": lambda y_true, y_pred: geometric_mean_score(\n",
    "        y_true, y_pred, average=\"macro\"\n",
    "    ),\n",
    "    \"micro_geometric_mean\": lambda y_true, y_pred: geometric_mean_score(\n",
    "        y_true, y_pred, average=\"micro\"\n",
    "    ),\n",
    "    # the below require .predict_proba output\n",
    "    \"auroc_macro_ovo\": lambda y_true, y_score: roc_auc_score(\n",
    "        y_true, y_score, average=\"macro\", multi_class=\"ovo\"\n",
    "    ),\n",
    "    \"auroc_macro_ovr\": lambda y_true, y_score: roc_auc_score(\n",
    "        y_true, y_score, average=\"macro\", multi_class=\"ovr\"\n",
    "    ),\n",
    "    \"auroc_weighted_ovo\": lambda y_true, y_score: roc_auc_score(\n",
    "        y_true, y_score, average=\"weighted\", multi_class=\"ovo\"\n",
    "    ),\n",
    "    \"auroc_weighted_ovr\": lambda y_true, y_score: roc_auc_score(\n",
    "        y_true, y_score, average=\"weighted\", multi_class=\"ovr\"\n",
    "    ),\n",
    "    \"log_loss\": log_loss,\n",
    "    \"top_2_accuracy\": top_k_accuracy_score,\n",
    "    # the below must be adapted to multiclass and require .predict_proba output\n",
    "    \"average_precision_neg\": binary_scorer(\n",
    "        lambda y_true, y_score: average_precision_score(\n",
    "            y_true, y_score, average=\"weighted\"\n",
    "        ),\n",
    "        \"neg\",\n",
    "    ),\n",
    "    \"average_precision_neu\": binary_scorer(\n",
    "        lambda y_true, y_score: average_precision_score(\n",
    "            y_true, y_score, average=\"weighted\"\n",
    "        ),\n",
    "        \"neu\",\n",
    "    ),\n",
    "    \"average_precision_pos\": binary_scorer(\n",
    "        lambda y_true, y_score: average_precision_score(\n",
    "            y_true, y_score, average=\"weighted\"\n",
    "        ),\n",
    "        \"pos\",\n",
    "    ),\n",
    "    \"brier_loss_neg\": binary_scorer(brier_score_loss, \"neg\"),\n",
    "    \"brier_loss_neu\": binary_scorer(brier_score_loss, \"neu\"),\n",
    "    \"brier_loss_pos\": binary_scorer(brier_score_loss, \"pos\"),\n",
    "}\n",
    "PREDICT_PROBA_KEYS = {\n",
    "    \"auroc_macro_ovo\",\n",
    "    \"auroc_macro_ovr\",\n",
    "    \"auroc_weighted_ovo\",\n",
    "    \"auroc_weighted_ovr\",\n",
    "    \"log_loss\",\n",
    "    \"top_2_accuracy\",\n",
    "    \"average_precision_neg\",\n",
    "    \"average_precision_neu\",\n",
    "    \"average_precision_pos\",\n",
    "    \"brier_loss_neg\",\n",
    "    \"brier_loss_neu\",\n",
    "    \"brier_loss_pos\",\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"model\": [],\n",
    "    \"score_set\": [],\n",
    "    **{metric: [] for metric in METRICS.keys()},\n",
    "}\n",
    "\n",
    "GNB_FEATURES = [\"spherical-LDA1\", \"spherical-LDA2\"]\n",
    "\n",
    "\n",
    "def score_model(\n",
    "    model_name: str, score_set: str, data: pd.DataFrame, results: Dict[str, List]\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"Score a fitted model using METRICS and record in results\"\"\"\n",
    "    results[\"model\"].append(model_name)\n",
    "    results[\"score_set\"].append(score_set)\n",
    "    model = load_model(model_name)\n",
    "    df = data\n",
    "    if model_name == \"bagging_GNB\":\n",
    "        df = data.loc[:, GNB_FEATURES]\n",
    "    _ = gc_collect()\n",
    "    y_true_ = labels.loc[df.index, \"valence\"]\n",
    "    y_pred = model.predict(df)\n",
    "    y_prob = model.predict_proba(df)\n",
    "    dump(y_prob, f\"{OUT_FOLDER}/predictions/{model_name}_{score_set}.pkl\")\n",
    "    for metric, scorer in tqdm(METRICS.items()):\n",
    "        try:\n",
    "            results[metric].append(\n",
    "                scorer(y_true_, y_prob if metric in PREDICT_PROBA_KEYS else y_pred)\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            results[metric].append(np.nan)\n",
    "    return results\n",
    "\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in tqdm(MODELS):\n",
    "    results = score_model(model, \"ternary\", data, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "for metric in METRICS.keys():\n",
    "    print(\n",
    "        results_df.loc[\n",
    "            results_df[metric]\n",
    "            == (\n",
    "                results_df[metric].min()\n",
    "                if \"loss\" in metric\n",
    "                else results_df[metric].max()\n",
    "            ),\n",
    "            [\"model\", metric],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "for metric in METRICS.keys():\n",
    "    df = results_df.sort_values(by=metric, ascending=\"loss\" in metric)\n",
    "    g = sns.catplot(\n",
    "        x=metric,\n",
    "        y=\"model\",\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        linewidth=0,\n",
    "        palette=\"colorblind\",\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    _ = ax.bar_label(ax.containers[0])\n",
    "    sns.despine(left=True, top=True, right=True)\n",
    "    _ = plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    del df\n",
    "    del g\n",
    "    del ax\n",
    "    _ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By speaker gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in tqdm(MODELS):\n",
    "    results = score_model(\n",
    "        model, \"m_speaker\", data.loc[labels.speaker_gender == \"m\", :], results\n",
    "    )\n",
    "for model in tqdm(MODELS):\n",
    "    results = score_model(\n",
    "        model, \"f_speaker\", data.loc[labels.speaker_gender == \"f\", :], results\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.loc[results_df.score_set.isin({\"f_speaker\", \"m_speaker\"}), :]\n",
    "for metric in METRICS.keys():\n",
    "    df = results_df.sort_values(by=metric, ascending=\"loss\" in metric)\n",
    "    g = sns.catplot(\n",
    "        x=\"model\",\n",
    "        y=metric,\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        hue=\"score_set\",\n",
    "        hue_order=[\"f_speaker\", \"m_speaker\"],\n",
    "        palette=\"colorblind\",\n",
    "        linewidth=0,\n",
    "        height=4,\n",
    "        aspect=3,\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.despine(top=True, bottom=True, right=True)\n",
    "    _ = plt.xlabel(\"\")\n",
    "    _ = plt.ylabel(\"\")\n",
    "    _ = plt.suptitle(metric, y=1.1)\n",
    "    _ = plt.legend(\n",
    "        bbox_to_anchor=(0.5, 1.15), loc=\"upper center\", ncol=2, frameon=False\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    for container in ax.containers:\n",
    "        _ = ax.bar_label(container)\n",
    "    plt.show()\n",
    "    del df\n",
    "    del g\n",
    "    del ax\n",
    "    _ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the first prototypes, it looks like the new prototypes are better with male speakers by nearly all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is roughly equivalent to the data source of the holdout sets. Note that x4nth055_SER_custom contains one sample in English, which is omitted in this section's analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in tqdm(MODELS):\n",
    "    results = score_model(model, \"arb\", data.loc[labels.lang1 == \"arb\", :], results)\n",
    "for model in tqdm(MODELS):\n",
    "    results = score_model(model, \"deu\", data.loc[labels.lang1 == \"deu\", :], results)\n",
    "for model in tqdm(MODELS):\n",
    "    results = score_model(model, \"kor\", data.loc[labels.lang1 == \"kor\", :], results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Arabic dataset only has two emotions (happy and neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.loc[results_df.score_set.isin({\"arb\", \"deu\", \"kor\"}), :]\n",
    "for metric in METRICS.keys():\n",
    "    df = results_df.sort_values(by=metric, ascending=\"loss\" in metric)\n",
    "    g = sns.catplot(\n",
    "        x=\"model\",\n",
    "        y=metric,\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        hue=\"score_set\",\n",
    "        hue_order=[\"arb\", \"deu\", \"kor\"],\n",
    "        palette=\"colorblind\",\n",
    "        linewidth=0,\n",
    "        height=4,\n",
    "        aspect=3,\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.despine(top=True, bottom=True, right=True)\n",
    "    _ = plt.xlabel(\"\")\n",
    "    _ = plt.ylabel(\"\")\n",
    "    _ = plt.suptitle(metric, y=1.1)\n",
    "    plt.legend(bbox_to_anchor=(0.5, 1.15), loc=\"upper center\", ncol=3, frameon=False)\n",
    "    ax = plt.gca()\n",
    "    for container in ax.containers:\n",
    "        _ = ax.bar_label(container)\n",
    "    plt.show()\n",
    "    del df\n",
    "    del g\n",
    "    del ax\n",
    "    _ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By valence (OvR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for valence in (\"neg\", \"neu\", \"pos\"):\n",
    "    for model in tqdm(MODELS):\n",
    "        results[\"model\"].append(model)\n",
    "        results[\"score_set\"].append(valence)\n",
    "        _ = model\n",
    "        df = data.loc[:, GNB_FEATURES] if model == \"bagging_GNB\" else data\n",
    "        model = load_model(model)\n",
    "        y_pred = model.predict(df)\n",
    "        y_prob = model.predict_proba(df)\n",
    "        dump(y_prob, f\"{OUT_FOLDER}/predictions/{_}_{valence}.pkl\")\n",
    "        del df\n",
    "        del model\n",
    "        _ = gc_collect()\n",
    "        for metric, scorer in tqdm(METRICS.items()):\n",
    "            if \"average_precision\" in metric or \"brier_score\" in metric:\n",
    "                if metric[-3:] != valence:\n",
    "                    results[metric].append(np.nan)\n",
    "                    continue\n",
    "            results[metric].append(\n",
    "                binary_scorer(scorer, valence)(\n",
    "                    y_true, y_prob if metric in PREDICT_PROBA_KEYS else y_pred\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.loc[\n",
    "    results_df.score_set.isin({\"ternary\", \"neg\", \"neu\", \"pos\"}), :\n",
    "]\n",
    "for metric in METRICS.keys():\n",
    "    df = results_df.sort_values(by=metric, ascending=\"loss\" in metric)\n",
    "    g = sns.catplot(\n",
    "        x=\"model\",\n",
    "        y=metric,\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        hue=\"score_set\",\n",
    "        hue_order=[\"ternary\", \"neg\", \"neu\", \"pos\"],\n",
    "        palette=\"colorblind\",\n",
    "        linewidth=0,\n",
    "        height=4,\n",
    "        aspect=3,\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.despine(top=True, bottom=True, right=True)\n",
    "    _ = plt.xlabel(\"\")\n",
    "    _ = plt.ylabel(\"\")\n",
    "    _ = plt.suptitle(metric, y=1.1)\n",
    "    _ = plt.legend(\n",
    "        bbox_to_anchor=(0.5, 1.15), loc=\"upper center\", ncol=4, frameon=False\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    for container in ax.containers:\n",
    "        _ = ax.bar_label(container)\n",
    "    plt.show()\n",
    "    del df\n",
    "    del g\n",
    "    del ax\n",
    "    _ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One English sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a positive sample, so 2 would be a correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = data.loc[labels.lang1 == \"eng\", :]\n",
    "for model in MODELS:\n",
    "    df = df_\n",
    "    if \"GNB\" in model:\n",
    "        df = df_.loc[:, GNB_FEATURES]\n",
    "    print(f\"{model} predicted {load_model(model).predict(df)}\")\n",
    "del df\n",
    "_ = gc_collect()\n",
    "# correct prediction is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = results_df.columns.astype(str)\n",
    "results_df.reset_index(drop=True).to_csv(f\"{OUT_FOLDER}/holdout_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How surprising to see such poor performance distinguishing negative samples.\n",
    "\n",
    "Overall, I am disappointed. There is a lot of room for improvement and many new approaches we could take with the data:\n",
    "* resampling\n",
    "* augmentation\n",
    "* more FRILL-based engineering\n",
    "* low-level audio features\n",
    "* spectrograms/MFCCs\n",
    "* deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* best AUROC: 64.1% (ridge)\n",
    "* best accuracy: 50.7% (bagging_GNB)\n",
    "* best log loss: 1.139 (voting_gnb_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll probably use the voting ensemble of bagging GNB and ridge classifier since it has the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Time elapsed since notebook_begin_time: {time() - notebook_begin_time} s\")\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^top](#Contents)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1781be99c106060f3abc0c9b91d3d379f24672894e2158d4b74304109955878"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
