{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Imports and configuration](#Imports-and-configuration)\n",
    "* [Load data](#Load-data)\n",
    "* [Strata](#Strata)\n",
    "* [Hyperparameters](#Hyperparameters)\n",
    "* [Models](#Models)\n",
    "* [Evaluate](#Evaluate)\n",
    "* [Discussion](#Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "After rounds of feature engineering, visualization & exploration, and tuning various aspects of the classification pipeline, we are about to create a benchmark prototype classifier using RandomForestClassifier. In this notebook, we perform a grid search over n_estimators using out-of-bag accuracy instead of cross validation. Other hyperparameters are based on previous 5-fold cross-validation using a related set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "notebook_begin_time = time()\n",
    "\n",
    "# set random seeds\n",
    "\n",
    "from os import environ\n",
    "from random import seed as random_seed\n",
    "from numpy.random import seed as np_seed\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "\n",
    "def reset_seeds(seed: int) -> None:\n",
    "    \"\"\"Utility function for resetting random seeds\"\"\"\n",
    "    environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random_seed(seed)\n",
    "    np_seed(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "\n",
    "reset_seeds(SEED := 2021)\n",
    "del environ\n",
    "del random_seed\n",
    "del np_seed\n",
    "del set_seed\n",
    "del reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extensions\n",
    "%load_ext autotime\n",
    "%load_ext lab_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.35 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utility\n",
    "from joblib import dump\n",
    "from gc import collect as gc_collect\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# faster\n",
    "import swifter\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "del patch_sklearn\n",
    "\n",
    "# typing\n",
    "from typing import List, Dict\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score\n",
    "\n",
    "# other sklearn\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedGroupKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# display outputs w/o print calls\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "del InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "# Location of FRILL .feather files\n",
    "FRILL_FEATHERS_FOLDER = \"../1.0-mic-extract_FRILL_embeddings\"\n",
    "\n",
    "# Location of pre-final features\n",
    "FEATURES_FOLDER = \"../19.0-mic-extract_FRILL-based_features_from_full_data\"\n",
    "\n",
    "# Location where this notebook will output\n",
    "DATA_OUT_FOLDER = \".\"\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 353 ms\n"
     ]
    }
   ],
   "source": [
    "def load_labels() -> pd.DataFrame:\n",
    "    \"\"\"Load just the labels\"\"\"\n",
    "    keep_columns = [\n",
    "        \"id\",\n",
    "        \"source\",\n",
    "        \"speaker_id\",\n",
    "        \"speaker_gender\",\n",
    "        \"emo\",\n",
    "        \"valence\",\n",
    "        \"lang1\",\n",
    "        \"length\",\n",
    "    ]\n",
    "    labels = pd.concat(\n",
    "        (\n",
    "            pd.read_feather(\n",
    "                f\"{FRILL_FEATHERS_FOLDER}/dev_labels.feather\", columns=keep_columns\n",
    "            ),\n",
    "            pd.read_feather(\n",
    "                f\"{FRILL_FEATHERS_FOLDER}/nondev_labels.feather\", columns=keep_columns\n",
    "            ),\n",
    "        )\n",
    "    ).set_index(\"id\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_data(unscaled=False) -> pd.DataFrame:\n",
    "    \"\"\"Loads the FRILL-based features\"\"\"\n",
    "    if unscaled:\n",
    "        df = pd.read_feather(\n",
    "            f\"{FEATURES_FOLDER}/unscaled_features_ready_for_selection.feather\"\n",
    "        ).set_index(\"id\")\n",
    "    else:\n",
    "        df = pd.read_feather(\n",
    "            f\"{FEATURES_FOLDER}/features_ready_for_selection.feather\"\n",
    "        ).set_index(\"id\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "data = load_data(unscaled=True)\n",
    "labels = load_labels()\n",
    "y_true = labels.valence\n",
    "gnb_features = [\"spherical-LDA1\", \"spherical-LDA2\"]\n",
    "assert all(data.index == labels.index)\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "UInt64Index: 86752 entries, 0 to 87363\n",
      "Columns: 118 entries, theta_LDA1+LDA2 to LDA-ocSVM_poly6_pos\n",
      "dtypes: float64(118)\n",
      "memory usage: 78.8 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "UInt64Index: 86752 entries, 0 to 87363\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   source          86752 non-null  category\n",
      " 1   speaker_id      86752 non-null  category\n",
      " 2   speaker_gender  86752 non-null  category\n",
      " 3   emo             86752 non-null  category\n",
      " 4   valence         86752 non-null  int8    \n",
      " 5   lang1           86752 non-null  category\n",
      " 6   length          86752 non-null  category\n",
      "dtypes: category(6), int8(1)\n",
      "memory usage: 1.4 MB\n",
      "time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "labels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dask Apply: 100%|██████████| 16/16 [00:03<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 7\n",
    "\n",
    "# fields are concatentated for quick permutation omitting non-existent combos\n",
    "strata = labels.loc[\n",
    "    :, [\"source\", \"speaker_gender\", \"emo\", \"valence\", \"lang1\", \"length\"]\n",
    "]\n",
    "strata.valence = strata.valence.astype(str)\n",
    "strata = strata.swifter.apply(\"\".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "# utility function for identifying strata with only i occurences\n",
    "def get_solo(i: int, strata_: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Given a series of stratum memberships, return a shuffled array of strata with only i members.\"\"\"\n",
    "    return np.unique(\n",
    "        strata_.loc[\n",
    "            strata_.isin(\n",
    "                (strata_counts := strata_.value_counts())\n",
    "                .where(strata_counts == i)\n",
    "                .dropna()\n",
    "                .index\n",
    "            )\n",
    "        ]\n",
    "        .sample(frac=1, random_state=SEED)\n",
    "        .values\n",
    "    )\n",
    "\n",
    "\n",
    "# get solos, print stuff\n",
    "def get_onlys(\n",
    "    strata_: pd.Series, print_me: str = \"\", n_splits: int = N_SPLITS\n",
    ") -> List[Dict[int, np.ndarray]]:\n",
    "    \"\"\"Optinally prints something and returns calls of get_solo on strata_ in a list\"\"\"\n",
    "    print(print_me)\n",
    "    solos = []\n",
    "    for i in range(1, n_splits):\n",
    "        solo: np.ndarray = get_solo(i, strata_)\n",
    "        print(f\"only {i}:\", (_ := solo.size))\n",
    "        if _:  # >= 1 strata with only i samples\n",
    "            solos.append({i: solo})\n",
    "    return solos\n",
    "\n",
    "\n",
    "def process_strata(strata: pd.Series, n_splits: int = N_SPLITS) -> pd.Series:\n",
    "    \"\"\"Corrects strata membership column according to n_splits\"\"\"\n",
    "\n",
    "    count = get_onlys_calls = 0\n",
    "\n",
    "    while onlys := get_onlys(\n",
    "        strata,\n",
    "        print_me=f\"merge passes performed: {get_onlys_calls}\",\n",
    "        n_splits=n_splits,\n",
    "    ):\n",
    "        get_onlys_calls += 1\n",
    "        if len(onlys) == 1:\n",
    "            last = onlys[0]\n",
    "            strata_to_merge: np.ndarray = list(last.values())[0]\n",
    "            only_key = list(last.keys())[0]\n",
    "            tuplet_size = n_splits // only_key + (1 if n_splits % only_key else 0)\n",
    "            # perform tuplet merge\n",
    "            interval = len(strata_to_merge) // n_splits\n",
    "            for strata_tuplet in zip(\n",
    "                *[\n",
    "                    strata_to_merge[interval * i : interval * (i + 1)]\n",
    "                    for i in range(tuplet_size)\n",
    "                ]\n",
    "            ):\n",
    "                strata = strata.replace(strata_tuplet, f\"stratum_group_{count}\")\n",
    "                count += 1\n",
    "            remainder = strata_to_merge[tuplet_size * interval :]\n",
    "            if len(remainder) == 1:\n",
    "                # process remainder unmatched\n",
    "                n = n_splits\n",
    "                strata_counts = strata.value_counts()\n",
    "                while not (candidates := strata_counts.loc[strata_counts == n]).size:\n",
    "                    n += 1\n",
    "                strata = strata.replace(\n",
    "                    [remainder[0], candidates.sample(n=1, random_state=SEED).index[0]],\n",
    "                    f\"stratum_group_{count}\",\n",
    "                )\n",
    "                count += 1\n",
    "            else:\n",
    "                # self-pair last\n",
    "                remainder = remainder.tolist()\n",
    "                while len(remainder) >= 2:\n",
    "                    strata = strata.replace(\n",
    "                        (remainder.pop(), remainder.pop()), f\"stratum_group_{count}\"\n",
    "                    )\n",
    "                    count += 1\n",
    "        else:\n",
    "            pop_onlys = lambda _: list(onlys.pop(_).values())[0].tolist()\n",
    "            while len(onlys) >= 2:\n",
    "                # pop the ends\n",
    "                shortside = pop_onlys(0)\n",
    "                longside = pop_onlys(-1)\n",
    "                # merge until one end empty\n",
    "                while shortside and longside:\n",
    "                    strata = strata.replace(\n",
    "                        (shortside.pop(), longside.pop()), f\"stratum_group_{count}\"\n",
    "                    )\n",
    "                    count += 1\n",
    "            if onlys:\n",
    "                # self-pair middle\n",
    "                remainder = pop_onlys(0)\n",
    "                while len(remainder) >= 2:\n",
    "                    strata = strata.replace(\n",
    "                        (remainder.pop(), remainder.pop()), f\"stratum_group_{count}\"\n",
    "                    )\n",
    "                    count += 1\n",
    "    return strata\n",
    "\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge passes performed: 0\n",
      "only 1: 53\n",
      "only 2: 39\n",
      "only 3: 31\n",
      "only 4: 17\n",
      "only 5: 27\n",
      "only 6: 13\n",
      "merge passes performed: 1\n",
      "only 1: 40\n",
      "only 2: 12\n",
      "only 3: 14\n",
      "only 4: 0\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "merge passes performed: 2\n",
      "only 1: 26\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 20\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "merge passes performed: 3\n",
      "only 1: 6\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 20\n",
      "only 6: 0\n",
      "merge passes performed: 4\n",
      "only 1: 0\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 14\n",
      "only 6: 6\n",
      "merge passes performed: 5\n",
      "only 1: 0\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 8\n",
      "only 6: 0\n",
      "merge passes performed: 6\n",
      "only 1: 0\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 0\n",
      "only 6: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MELDmneu1engmedium    2905\n",
       "MELDfneu1engmedium    2452\n",
       "esdfsur0engmedium     1750\n",
       "esdfhap2engmedium     1750\n",
       "esdfneu1engmedium     1750\n",
       "                      ... \n",
       "stratum_group_33         7\n",
       "stratum_group_38         7\n",
       "stratum_group_1          7\n",
       "stratum_group_35         7\n",
       "stratum_group_6          7\n",
       "Length: 455, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "STRATA = process_strata(strata, n_splits=N_SPLITS)\n",
    "STRATA.value_counts()\n",
    "cross_validator = lambda: StratifiedGroupKFold(\n",
    "    n_splits=N_SPLITS, shuffle=True, random_state=SEED\n",
    ").split(X=data, y=STRATA, groups=labels.speaker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "rf_params = lambda: {\n",
    "    \"n_estimators\": 342,\n",
    "    \"criterion\": \"entropy\",\n",
    "    \"max_depth\": 17,\n",
    "    \"min_samples_split\": 9,\n",
    "    \"min_samples_leaf\": 4,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"max_leaf_nodes\": 2207,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": True,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": SEED,\n",
    "    \"warm_start\": False,\n",
    "    \"class_weight\": \"balanced_subsample\",\n",
    "}\n",
    "\n",
    "var_smoothing = 0.10556503264086932\n",
    "gnb_params = lambda: {\n",
    "    \"base_estimator\": GaussianNB(var_smoothing=var_smoothing),\n",
    "    \"n_estimators\": 23,\n",
    "    \"oob_score\": True,\n",
    "    \"warm_start\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": SEED,\n",
    "}\n",
    "\n",
    "calibration_params = lambda: {\n",
    "    \"method\": \"isotonic\",\n",
    "    \"cv\": list(cross_validator()),\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "alpha = 0.960072\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "gnb_data = data.loc[:, gnb_features]\n",
    "# plain_gnb = lambda: GaussianNB(var_smoothing=var_smoothing).fit(gnb_data, y_true)\n",
    "# bagging_gnb = lambda: BaggingClassifier(**gnb_params()).fit(gnb_data, y_true)\n",
    "# rf = lambda: RandomForestClassifier(**rf_params()).fit(data, y_true)\n",
    "ridge = lambda: CalibratedClassifierCV(\n",
    "    base_estimator=RidgeClassifier(alpha=alpha, random_state=SEED),\n",
    "    **calibration_params()\n",
    ").fit(data, y_true)\n",
    "voting = lambda: VotingClassifier(\n",
    "    estimators=[\n",
    "        (\n",
    "            \"ridge\",\n",
    "            CalibratedClassifierCV(\n",
    "                base_estimator=RidgeClassifier(alpha=alpha, random_state=SEED),\n",
    "                **calibration_params()\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"gnb\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\n",
    "                        \"select_features\",\n",
    "                        ColumnTransformer(\n",
    "                            transformers=[(\"selector\", \"passthrough\", gnb_features)],\n",
    "                            n_jobs=-1,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"clf\",\n",
    "                        CalibratedClassifierCV(\n",
    "                            base_estimator=BaggingClassifier(**gnb_params()),\n",
    "                            **calibration_params()\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    "    n_jobs=-1,\n",
    ").fit(data, y_true)\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13 s\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    # \"plain_GNB\": plain_gnb(),\n",
    "    # \"bagging_GNB\": bagging_gnb(),\n",
    "    # \"randomforest\": rf(),\n",
    "    \"voting_ensemble_gnb_ridge\": voting(),\n",
    "    \"ridge\": ridge(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a0d95d41d04c51a710de220290b8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['./prototypes/voting_ensemble_gnb_ridge.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./prototypes/ridge.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 276 ms\n"
     ]
    }
   ],
   "source": [
    "for model, estimator in tqdm(models.items()):\n",
    "    dump(estimator, f\"{DATA_OUT_FOLDER}/prototypes/{model}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post hoc calibrated ridge classifier quick train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed since notebook_begin_time: 32.32737708091736 s\n",
      "time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time elapsed since notebook_begin_time: {time() - notebook_begin_time} s\")\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1781be99c106060f3abc0c9b91d3d379f24672894e2158d4b74304109955878"
  },
  "kernelspec": {
   "display_name": "Python [conda env:capstone_pyspark_38] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
