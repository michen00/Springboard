{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Imports and configuration](#Imports-and-configuration)\n",
    "* [Load data](#Load-data)\n",
    "* [Strata](#Strata)\n",
    "* [Hyperparameters](#Hyperparameters)\n",
    "* [Models](#Models)\n",
    "* [Evaluate](#Evaluate)\n",
    "* [Discussion](#Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The chosen classifier is  supported by the feature extraction pipeline. The full pipeline is retrained on all available data including data previously utilized for holdout evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "notebook_begin_time = time()\n",
    "\n",
    "# set random seeds\n",
    "\n",
    "from os import environ\n",
    "from random import seed as random_seed\n",
    "from numpy.random import seed as np_seed\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "\n",
    "def reset_seeds(seed: int) -> None:\n",
    "    \"\"\"Utility function for resetting random seeds\"\"\"\n",
    "    environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random_seed(seed)\n",
    "    np_seed(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "\n",
    "reset_seeds(SEED := 2022)\n",
    "del environ\n",
    "del random_seed\n",
    "del np_seed\n",
    "del set_seed\n",
    "del reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extensions\n",
    "%load_ext autotime\n",
    "%load_ext lab_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.57 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utility\n",
    "from joblib import dump\n",
    "from gc import collect as gc_collect\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# faster\n",
    "import swifter\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "del patch_sklearn\n",
    "\n",
    "# typing\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "# other sklearn\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import BaggingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    LogisticRegressionCV,\n",
    "    RidgeClassifier,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# display outputs w/o print calls\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "del InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "# Location of pre-final features\n",
    "FEATURES_FOLDER = \".\"\n",
    "\n",
    "# Location where this notebook will output\n",
    "DATA_OUT_FOLDER = \".\"\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "def load_labels() -> pd.DataFrame:\n",
    "    \"\"\"Load just the labels\"\"\"\n",
    "    keep_columns = [\n",
    "        \"source\",\n",
    "        \"speaker_id\",\n",
    "        \"speaker_gender\",\n",
    "        \"emo\",\n",
    "        \"valence\",\n",
    "        \"lang1\",\n",
    "        \"length\",\n",
    "    ]\n",
    "\n",
    "    return pd.read_feather(\n",
    "        f\"{FEATURES_FOLDER}/final_labels.feather\", columns=keep_columns\n",
    "    )\n",
    "\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Loads the FRILL-based features\"\"\"\n",
    "    df = pd.read_feather(\"./scaled_features.feather\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "labels = load_labels().loc[data.index]\n",
    "y_true = labels.valence\n",
    "gnb_features = [\"spherical-LDA1\", \"spherical-LDA2\"]\n",
    "assert all(data.index == labels.index)\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88709 entries, 0 to 88708\n",
      "Data columns (total 41 columns):\n",
      " #   Column                                                                         Non-Null Count  Dtype  \n",
      "---  ------                                                                         --------------  -----  \n",
      " 0   LDA1                                                                           88709 non-null  float64\n",
      " 1   LDA2                                                                           88709 non-null  float64\n",
      " 2   ocLDA_neg                                                                      88709 non-null  float64\n",
      " 3   ocLDA_neu                                                                      88709 non-null  float64\n",
      " 4   ocLDA_pos                                                                      88709 non-null  float64\n",
      " 5   LDA-LOF_neg_20                                                                 88709 non-null  float64\n",
      " 6   LDA-LOF_neu_20                                                                 88709 non-null  float64\n",
      " 7   LDA-LOF_pos_20                                                                 88709 non-null  float64\n",
      " 8   LDA-ocSVM_sgdlinear_neg                                                        88709 non-null  float64\n",
      " 9   LDA-ocSVM_sgdlinear_neu                                                        88709 non-null  float64\n",
      " 10  LDA-ocSVM_sgdlinear_pos                                                        88709 non-null  float64\n",
      " 11  LDA-ocSVM_rbf_neg                                                              88709 non-null  float64\n",
      " 12  LDA-ocSVM_rbf_neu                                                              88709 non-null  float64\n",
      " 13  LDA-ocSVM_rbf_pos                                                              88709 non-null  float64\n",
      " 14  LDA-ocSVM_sigmoid_neg                                                          88709 non-null  float64\n",
      " 15  LDA-ocSVM_sigmoid_neu                                                          88709 non-null  float64\n",
      " 16  LDA-ocSVM_sigmoid_pos                                                          88709 non-null  float64\n",
      " 17  ocSVM_sgdlinear_neg                                                            88709 non-null  float64\n",
      " 18  ocSVM_sgdlinear_neu                                                            88709 non-null  float64\n",
      " 19  ocSVM_sgdlinear_pos                                                            88709 non-null  float64\n",
      " 20  ocSVM_sgdlinear_LDA1                                                           88709 non-null  float64\n",
      " 21  ocSVM_sgdlinear_LDA2                                                           88709 non-null  float64\n",
      " 22  spherical-LDA1                                                                 88709 non-null  float64\n",
      " 23  spherical-LDA2                                                                 88709 non-null  float64\n",
      " 24  spherical-ocLDA_neg                                                            88709 non-null  float64\n",
      " 25  spherical-ocLDA_neu                                                            88709 non-null  float64\n",
      " 26  spherical-ocLDA_pos                                                            88709 non-null  float64\n",
      " 27  theta_LDA1+LDA2                                                                88709 non-null  float64\n",
      " 28  theta_ocLDA_neg+ocLDA_neu+ocLDA_pos                                            88709 non-null  float64\n",
      " 29  phi_ocLDA_neg+ocLDA_neu+ocLDA_pos                                              88709 non-null  float64\n",
      " 30  theta_ocSVM_sgdlinear_LDA1+ocSVM_sgdlinear_LDA2                                88709 non-null  float64\n",
      " 31  theta_ocSVM_sgdlinear_neg+ocSVM_sgdlinear_neu+ocSVM_sgdlinear_pos              88709 non-null  float64\n",
      " 32  phi_ocSVM_sgdlinear_neg+ocSVM_sgdlinear_neu+ocSVM_sgdlinear_pos                88709 non-null  float64\n",
      " 33  theta_LDA-LOF_neg_20+LDA-LOF_neu_20+LDA-LOF_pos_20                             88709 non-null  float64\n",
      " 34  phi_LDA-LOF_neg_20+LDA-LOF_neu_20+LDA-LOF_pos_20                               88709 non-null  float64\n",
      " 35  theta_LDA-ocSVM_sgdlinear_neg+LDA-ocSVM_sgdlinear_neu+LDA-ocSVM_sgdlinear_pos  88709 non-null  float64\n",
      " 36  phi_LDA-ocSVM_sgdlinear_neg+LDA-ocSVM_sgdlinear_neu+LDA-ocSVM_sgdlinear_pos    88709 non-null  float64\n",
      " 37  theta_LDA-ocSVM_rbf_neg+LDA-ocSVM_rbf_neu+LDA-ocSVM_rbf_pos                    88709 non-null  float64\n",
      " 38  phi_LDA-ocSVM_rbf_neg+LDA-ocSVM_rbf_neu+LDA-ocSVM_rbf_pos                      88709 non-null  float64\n",
      " 39  theta_LDA-ocSVM_sigmoid_neg+LDA-ocSVM_sigmoid_neu+LDA-ocSVM_sigmoid_pos        88709 non-null  float64\n",
      " 40  phi_LDA-ocSVM_sigmoid_neg+LDA-ocSVM_sigmoid_neu+LDA-ocSVM_sigmoid_pos          88709 non-null  float64\n",
      "dtypes: float64(41)\n",
      "memory usage: 27.7 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88709 entries, 0 to 88708\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   source          88709 non-null  object  \n",
      " 1   speaker_id      88709 non-null  object  \n",
      " 2   speaker_gender  88709 non-null  category\n",
      " 3   emo             88709 non-null  object  \n",
      " 4   valence         88709 non-null  int8    \n",
      " 5   lang1           88709 non-null  object  \n",
      " 6   length          86752 non-null  category\n",
      "dtypes: category(2), int8(1), object(4)\n",
      "memory usage: 3.0+ MB\n",
      "time: 56 ms\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "labels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dask Apply: 100%|██████████| 16/16 [00:02<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 8\n",
    "\n",
    "# fields are concatentated for quick permutation omitting non-existent combos\n",
    "strata = labels.loc[\n",
    "    :, [\"source\", \"speaker_gender\", \"emo\", \"valence\", \"lang1\", \"length\"]\n",
    "].astype(str)\n",
    "strata = strata.swifter.apply(\"\".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 108 ms\n"
     ]
    }
   ],
   "source": [
    "# utility function for identifying strata with only i occurences\n",
    "def get_solo(i: int, strata_: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Given a series of stratum memberships, return a shuffled array of strata with only i members.\"\"\"\n",
    "    return np.unique(\n",
    "        strata_.loc[\n",
    "            strata_.isin(\n",
    "                (strata_counts := strata_.value_counts())\n",
    "                .where(strata_counts == i)\n",
    "                .dropna()\n",
    "                .index\n",
    "            )\n",
    "        ]\n",
    "        .sample(frac=1, random_state=SEED)\n",
    "        .values\n",
    "    )\n",
    "\n",
    "\n",
    "# get solos, print stuff\n",
    "def get_onlys(\n",
    "    strata_: pd.Series, print_me: str = \"\", n_splits: int = N_SPLITS\n",
    ") -> List[Dict[int, np.ndarray]]:\n",
    "    \"\"\"Optinally prints something and returns calls of get_solo on strata_ in a list\"\"\"\n",
    "    print(print_me)\n",
    "    solos = []\n",
    "    for i in range(1, n_splits):\n",
    "        solo: np.ndarray = get_solo(i, strata_)\n",
    "        print(f\"only {i}:\", (_ := solo.size))\n",
    "        if _:  # >= 1 strata with only i samples\n",
    "            solos.append({i: solo})\n",
    "    return solos\n",
    "\n",
    "\n",
    "def process_strata(strata: pd.Series, n_splits: int = N_SPLITS) -> pd.Series:\n",
    "    \"\"\"Corrects strata membership column according to n_splits\"\"\"\n",
    "\n",
    "    count = get_onlys_calls = 0\n",
    "\n",
    "    while onlys := get_onlys(\n",
    "        strata,\n",
    "        print_me=f\"merge passes performed: {get_onlys_calls}\",\n",
    "        n_splits=n_splits,\n",
    "    ):\n",
    "        get_onlys_calls += 1\n",
    "        if len(onlys) == 1:\n",
    "            last = onlys[0]\n",
    "            strata_to_merge: np.ndarray = list(last.values())[0]\n",
    "            only_key = list(last.keys())[0]\n",
    "            tuplet_size = n_splits // only_key + (1 if n_splits % only_key else 0)\n",
    "            # perform tuplet merge\n",
    "            interval = len(strata_to_merge) // n_splits\n",
    "            for strata_tuplet in zip(\n",
    "                *[\n",
    "                    strata_to_merge[interval * i : interval * (i + 1)]\n",
    "                    for i in range(tuplet_size)\n",
    "                ]\n",
    "            ):\n",
    "                strata = strata.replace(strata_tuplet, f\"stratum_group_{count}\")\n",
    "                count += 1\n",
    "            remainder = strata_to_merge[tuplet_size * interval :]\n",
    "            if len(remainder) == 1:\n",
    "                # process remainder unmatched\n",
    "                n = n_splits\n",
    "                strata_counts = strata.value_counts()\n",
    "                while not (candidates := strata_counts.loc[strata_counts == n]).size:\n",
    "                    n += 1\n",
    "                strata = strata.replace(\n",
    "                    [remainder[0], candidates.sample(n=1, random_state=SEED).index[0]],\n",
    "                    f\"stratum_group_{count}\",\n",
    "                )\n",
    "                count += 1\n",
    "            else:\n",
    "                # self-pair last\n",
    "                remainder = remainder.tolist()\n",
    "                while len(remainder) >= 2:\n",
    "                    strata = strata.replace(\n",
    "                        (remainder.pop(), remainder.pop()), f\"stratum_group_{count}\"\n",
    "                    )\n",
    "                    count += 1\n",
    "        else:\n",
    "            pop_onlys = lambda _: list(onlys.pop(_).values())[0].tolist()\n",
    "            while len(onlys) >= 2:\n",
    "                # pop the ends\n",
    "                shortside = pop_onlys(0)\n",
    "                longside = pop_onlys(-1)\n",
    "                # merge until one end empty\n",
    "                while shortside and longside:\n",
    "                    strata = strata.replace(\n",
    "                        (shortside.pop(), longside.pop()), f\"stratum_group_{count}\"\n",
    "                    )\n",
    "                    count += 1\n",
    "            if onlys:\n",
    "                # self-pair middle\n",
    "                remainder = pop_onlys(0)\n",
    "                while len(remainder) >= 2:\n",
    "                    strata = strata.replace(\n",
    "                        (remainder.pop(), remainder.pop()), f\"stratum_group_{count}\"\n",
    "                    )\n",
    "                    count += 1\n",
    "    return strata\n",
    "\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge passes performed: 0\n",
      "only 1: 54\n",
      "only 2: 39\n",
      "only 3: 31\n",
      "only 4: 17\n",
      "only 5: 27\n",
      "only 6: 13\n",
      "only 7: 9\n",
      "merge passes performed: 1\n",
      "only 1: 45\n",
      "only 2: 26\n",
      "only 3: 4\n",
      "only 4: 1\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "only 7: 0\n",
      "merge passes performed: 2\n",
      "only 1: 44\n",
      "only 2: 22\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 5\n",
      "only 6: 0\n",
      "only 7: 0\n",
      "merge passes performed: 3\n",
      "only 1: 39\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 11\n",
      "only 5: 0\n",
      "only 6: 5\n",
      "only 7: 0\n",
      "merge passes performed: 4\n",
      "only 1: 34\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 1\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "only 7: 5\n",
      "merge passes performed: 5\n",
      "only 1: 29\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 1\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "only 7: 0\n",
      "merge passes performed: 6\n",
      "only 1: 28\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 1\n",
      "only 6: 0\n",
      "only 7: 0\n",
      "merge passes performed: 7\n",
      "only 1: 27\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 0\n",
      "only 6: 1\n",
      "only 7: 0\n",
      "merge passes performed: 8\n",
      "only 1: 26\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "only 7: 1\n",
      "merge passes performed: 9\n",
      "only 1: 25\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "only 7: 0\n",
      "merge passes performed: 10\n",
      "only 1: 0\n",
      "only 2: 0\n",
      "only 3: 0\n",
      "only 4: 0\n",
      "only 5: 0\n",
      "only 6: 0\n",
      "only 7: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MELDmneu1engmedium     2905\n",
       "MELDfneu1engmedium     2452\n",
       "esdmhap2cmnmedium      1750\n",
       "esdmneu1cmnmedium      1750\n",
       "esdfsur0engmedium      1750\n",
       "                       ... \n",
       "BAUM2fcon0engmedium       8\n",
       "stratum_group_27          8\n",
       "BAUM1fdis0turlong         8\n",
       "stratum_group_22          8\n",
       "stratum_group_30          8\n",
       "Length: 467, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "STRATA = process_strata(strata, n_splits=N_SPLITS)\n",
    "STRATA.value_counts()\n",
    "cross_validator = lambda: StratifiedGroupKFold(\n",
    "    n_splits=N_SPLITS, shuffle=True, random_state=SEED\n",
    ").split(X=data, y=STRATA, groups=labels.speaker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "var_smoothing = 0.035464512306105304\n",
    "alpha = 249945.69701031796\n",
    "logreg_C2f = 0.014997851107584437\n",
    "\n",
    "ridge_params = lambda: {\n",
    "    \"alpha\": alpha,\n",
    "    \"random_state\": SEED,\n",
    "}\n",
    "\n",
    "\n",
    "def logreg_params(logreg_C: Union[None, float] = 10.0) -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"return logistic regression paramaters\"\"\"\n",
    "    logreg_params = {\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"tol\": 1e-5,\n",
    "        \"max_iter\": 1000000,\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": SEED,\n",
    "    }\n",
    "    if logreg_C:\n",
    "        logreg_params[\"C\"] = logreg_C\n",
    "    return logreg_params\n",
    "\n",
    "\n",
    "gnb_params = lambda: {\n",
    "    \"base_estimator\": GaussianNB(var_smoothing=var_smoothing),\n",
    "    \"n_estimators\": 50,\n",
    "    \"warm_start\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": SEED,\n",
    "}\n",
    "\n",
    "calibration_params = lambda: {\n",
    "    \"method\": \"isotonic\",\n",
    "    \"cv\": list(cross_validator()),\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "stacked_pass = lambda: StackingClassifier(\n",
    "    estimators=[\n",
    "        (\n",
    "            \"ridge\",\n",
    "            RidgeClassifier(**ridge_params()),\n",
    "        ),\n",
    "        (\n",
    "            \"gnb\",\n",
    "            BaggingClassifier(**gnb_params()),\n",
    "        ),\n",
    "    ],\n",
    "    final_estimator=LogisticRegressionCV(\n",
    "        scoring=\"neg_log_loss\",\n",
    "        **logreg_params(None),\n",
    "    ),\n",
    "    cv=list(cross_validator()),\n",
    "    n_jobs=-1,\n",
    "    passthrough=True,\n",
    "    verbose=1,\n",
    ").fit(data, y_true)\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "models[\"stacked_pass\"] = stacked_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fe595181884518a258ebbaf6326948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['./stacked_pass.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "for model, estimator in tqdm(models.items()):\n",
    "    dump(estimator, f\"{DATA_OUT_FOLDER}/{model}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to deploy this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed since notebook_begin_time: 204.96057963371277 s\n",
      "time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time elapsed since notebook_begin_time: {time() - notebook_begin_time} s\")\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1781be99c106060f3abc0c9b91d3d379f24672894e2158d4b74304109955878"
  },
  "kernelspec": {
   "display_name": "Python [conda env:capstone_pyspark_38] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
