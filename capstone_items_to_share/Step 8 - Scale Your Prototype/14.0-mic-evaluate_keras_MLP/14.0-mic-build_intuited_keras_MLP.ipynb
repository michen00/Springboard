{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Imports and configuration](#Imports-and-configuration)\n",
    "* [Setup](#Setup)\n",
    "* [Models](#Models)\n",
    "* [Test harness](#Test-harness)\n",
    "* [Results](#Results)\n",
    "* [Discussion](#Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we build and test a `keras` dense net using intuited configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "notebook_begin_time = time()\n",
    "\n",
    "# set random seeds\n",
    "\n",
    "from os import environ\n",
    "from random import seed as random_seed\n",
    "from numpy.random import seed as np_seed\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "\n",
    "def reset_seeds(seed: int) -> None:\n",
    "    \"\"\"Utility function for resetting random seeds\"\"\"\n",
    "    environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random_seed(seed)\n",
    "    np_seed(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "\n",
    "reset_seeds(SEED := 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extensions\n",
    "%load_ext autotime\n",
    "%load_ext lab_black\n",
    "%load_ext nb_black\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utility\n",
    "from collections import namedtuple\n",
    "from gc import collect as gc_collect\n",
    "\n",
    "# typing\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "# faster pandas & sklearn\n",
    "import swifter\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "del patch_sklearn\n",
    "\n",
    "# metrics\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# keras & tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    AlphaDropout,\n",
    "    BatchNormalization,\n",
    "    Dense,\n",
    ")\n",
    "\n",
    "# visualization\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"ticks\")\n",
    "%matplotlib inline\n",
    "\n",
    "# display outputs w/o print calls\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "del InteractiveShell\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "del warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of cross validation .feather files\n",
    "FRILL_FEATHERS_FOLDER = \"../10.0-mic-prepare_train-test_splits_on_full_data\"\n",
    "\n",
    "# Location where this notebook will output\n",
    "DATA_OUT_FOLDER = \".\"\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this section, we define some structures to keep track of models and scores.\n",
    "\n",
    "The next cell defines a Python dictionary to record the results of cross validation such that the results may be easily cast to a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CASES = {\"negative\", \"neutral\", \"ternary\", \"ternary_negneu\"}\n",
    "\n",
    "METRICS = (\n",
    "    \"balanced_accuracy\",\n",
    "    \"balanced_accuracy_adjusted\",\n",
    "    \"f1\",\n",
    "    \"geometric_mean\",\n",
    "    \"roc_auc\",\n",
    "    \"fit_time\",\n",
    "    \"predict_time\",\n",
    ")\n",
    "\n",
    "# to be cast to pd.DataFrame\n",
    "make_results = lambda: {key: [] for key in {\"model_name\", \"case\", *METRICS}}\n",
    "results_ = make_results()\n",
    "\n",
    "AvgScores = namedtuple(\"AvgScores\", METRICS)\n",
    "\n",
    "\n",
    "def store_result(\n",
    "    where: Dict[str, List],\n",
    "    case: str,\n",
    "    model_name: str,\n",
    "    avg_scores: AvgScores,\n",
    ") -> None:\n",
    "    \"Appends a model's scores and fit/predict times to the results dict.\"\n",
    "    for attribute, value in {\n",
    "        \"model_name\": model_name,\n",
    "        \"case\": case,\n",
    "        **avg_scores._asdict(),\n",
    "    }.items():\n",
    "        where[attribute].append(value)\n",
    "\n",
    "\n",
    "def create_results_df(results_dict: Dict[str, List]) -> pd.DataFrame:\n",
    "    \"\"\"Create a results dataframe from the results dictionary\"\"\"\n",
    "    df = (\n",
    "        pd.DataFrame(results_)\n",
    "        .sort_values(by=[\"model_name\"], ascending=True)\n",
    "        .sort_values(by=[\"fit_time\", \"predict_time\"], ascending=True)\n",
    "        .sort_values(\n",
    "            by=[\n",
    "                \"balanced_accuracy\",\n",
    "                \"balanced_accuracy_adjusted\",\n",
    "                \"geometric_mean\",\n",
    "                \"f1\",\n",
    "                \"roc_auc\",\n",
    "            ],\n",
    "            ascending=False,\n",
    "        )\n",
    "        .sort_values(by=[\"case\"], ascending=True)\n",
    "        .reset_index(drop=True)\n",
    "    )[\n",
    "        [  # selection order\n",
    "            \"model_name\",\n",
    "            \"case\",\n",
    "            \"balanced_accuracy\",\n",
    "            \"balanced_accuracy_adjusted\",\n",
    "            \"f1\",\n",
    "            \"geometric_mean\",\n",
    "            \"roc_auc\",\n",
    "            \"fit_time\",\n",
    "            \"predict_time\",\n",
    "        ]\n",
    "    ]\n",
    "    df.loc[:, \"case\"] = df.loc[:, \"case\"].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ = {key: [] for key in {\"model\", \"case\", \"fold\", \"y_pred\"}}\n",
    "\n",
    "Prediction = namedtuple(\"Prediction\", predictions_.keys())\n",
    "\n",
    "\n",
    "def store_prediction(where: Dict[str, List], prediction: Prediction) -> None:\n",
    "    \"Records the y_pred of a classifier on a fold\"\n",
    "    for k, v in prediction._asdict().items():\n",
    "        where[k].append(v)\n",
    "\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "In this section, we set up the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MLP(case: str, output_bias: float, print_summary: bool = True) -> Sequential:\n",
    "    \"\"\"Return a prepared keras MLP\"\"\"\n",
    "    if \"ternary\" in case:\n",
    "        out_nodes, final_activation, loss, accuracy = (\n",
    "            3,\n",
    "            \"softmax\",\n",
    "            \"sparse_categorical_crossentropy\",\n",
    "            \"sparse_categorical_accuracy\",\n",
    "        )\n",
    "    else:\n",
    "        out_nodes, final_activation, loss, accuracy = (\n",
    "            1,\n",
    "            \"sigmoid\",\n",
    "            \"binary_crossentropy\",\n",
    "            \"binary_accuracy\",\n",
    "        )\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            BatchNormalization(input_dim=2048),\n",
    "            AlphaDropout(0.1, seed=SEED),\n",
    "            Dense(\n",
    "                1024,\n",
    "                kernel_initializer=\"lecun_normal\",\n",
    "                activation=\"selu\",\n",
    "                activity_regularizer=\"l2\",\n",
    "            ),\n",
    "            AlphaDropout(0.1, seed=SEED),\n",
    "            Dense(\n",
    "                1024,\n",
    "                kernel_initializer=\"lecun_normal\",\n",
    "                activation=\"selu\",\n",
    "                activity_regularizer=\"l2\",\n",
    "            ),\n",
    "            AlphaDropout(0.1, seed=SEED),\n",
    "            Dense(\n",
    "                512,\n",
    "                kernel_initializer=\"lecun_normal\",\n",
    "                activation=\"selu\",\n",
    "                activity_regularizer=\"l2\",\n",
    "            ),\n",
    "            AlphaDropout(0.1, seed=SEED),\n",
    "            Dense(\n",
    "                256,\n",
    "                kernel_initializer=\"lecun_normal\",\n",
    "                activation=\"selu\",\n",
    "                activity_regularizer=\"l2\",\n",
    "            ),\n",
    "            AlphaDropout(0.1, seed=SEED),\n",
    "            Dense(\n",
    "                128,\n",
    "                kernel_initializer=\"lecun_normal\",\n",
    "                activation=\"selu\",\n",
    "                activity_regularizer=\"l2\",\n",
    "            ),\n",
    "            AlphaDropout(0.1, seed=SEED),\n",
    "            Dense(\n",
    "                50,\n",
    "                kernel_initializer=\"lecun_normal\",\n",
    "                activation=\"selu\",\n",
    "                activity_regularizer=\"l2\",\n",
    "            ),\n",
    "            AlphaDropout(0.1, seed=SEED),\n",
    "            Dense(\n",
    "                out_nodes,\n",
    "                activation=final_activation,\n",
    "                bias_initializer=output_bias,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "        metrics=accuracy,\n",
    "    )\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test harness\n",
    "\n",
    "This section defines functions for evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_y: Dict[str, Callable] = {\n",
    "    \"negative\": lambda y: ((y - 1) // 2) * (-1),\n",
    "    \"neutral\": lambda y: y % 2,\n",
    "    \"positive\": lambda y: y // 2,\n",
    "    \"ternary\": lambda y: np.squeeze(y),\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model_base: str, case: str, track_fold_time: bool = True\n",
    ") -> AvgScores:\n",
    "    \"\"\"Evaluate a model with cross validation on prepared folds.\"\"\"\n",
    "    (\n",
    "        fit_times,\n",
    "        predict_times,\n",
    "        balanced_accuracy,\n",
    "        balanced_accuracy_adjusted,\n",
    "        f1,\n",
    "        geometric_mean,\n",
    "        roc_auc,\n",
    "    ) = ([] for _ in range(7))\n",
    "    fold_num = 0\n",
    "    while True:\n",
    "        if track_fold_time:\n",
    "            fold_begin = time()\n",
    "\n",
    "        def read_feather_cv(xy_set: str) -> pd.DataFrame:\n",
    "            \"\"\"Helper function for reading split data\"\"\"\n",
    "            path_prefix = f\"{FRILL_FEATHERS_FOLDER}/cv_{fold_num}/{xy_set}\"\n",
    "            if \"train\" in xy_set:\n",
    "                return pd.read_feather(f\"{path_prefix}_LOF.feather\")\n",
    "            else:\n",
    "                return pd.read_feather(f\"{path_prefix}_untransformed.feather\")\n",
    "\n",
    "        # load training data\n",
    "        try:\n",
    "            X_train: pd.DataFrame = read_feather_cv(\"X_train\")\n",
    "            y_train: np.ndarray = read_feather_cv(\"y_train\").values\n",
    "        except FileNotFoundError:\n",
    "            break\n",
    "\n",
    "        # recode/reformat y_train\n",
    "        recoder = recode_y[case]\n",
    "        y_train = recoder(y_train)\n",
    "\n",
    "        # load testing data\n",
    "        X_test = read_feather_cv(\"X_test\")\n",
    "        y_test = np.squeeze(read_feather_cv(\"y_test\").values).astype(np.int8)\n",
    "\n",
    "        # initialize model with output layer bias\n",
    "        if multiclass := \"ternary\" in case:\n",
    "            zeros, ones, twos = np.bincount(y_test)\n",
    "            init_bias = tf.keras.initializers.Constant(\n",
    "                np.squeeze(\n",
    "                    [\n",
    "                        np.log([zeros / (ones + twos)]),\n",
    "                        np.log([ones / (zeros + twos)]),\n",
    "                        np.log([twos / (zeros + ones)]),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            del twos\n",
    "        else:\n",
    "            # recode y_test\n",
    "            y_test = recoder(y_test)\n",
    "            del recoder\n",
    "            _ = gc_collect()\n",
    "            zeros, ones = np.bincount(y_test)\n",
    "            init_bias = tf.keras.initializers.Constant(np.log([ones / zeros]))\n",
    "        del ones\n",
    "        del zeros\n",
    "        _ = gc_collect()\n",
    "\n",
    "        model = make_MLP(case, output_bias=init_bias)\n",
    "        del init_bias\n",
    "        _ = gc_collect()\n",
    "\n",
    "        # fit\n",
    "        reset_seeds(SEED)\n",
    "        begin = time()\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=16,\n",
    "            epochs=100,\n",
    "            # validation_data=(X_test, y_test),\n",
    "            validation_split=0.1,\n",
    "            callbacks=EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        )\n",
    "        end = time()\n",
    "        del X_train\n",
    "        del y_train\n",
    "        _ = gc_collect()\n",
    "        fit_times.append(end - begin)\n",
    "        print(f\"fitted in {end - begin:.2f} s\")\n",
    "\n",
    "        pd.DataFrame(history.history).plot(\n",
    "            cmap=ListedColormap(sns.color_palette(\"colorblind\").as_hex())\n",
    "        )\n",
    "        plt.title(\"accuracy and loss by epochs\")\n",
    "        plt.legend(frameon=False)\n",
    "        sns.despine(**dict.fromkeys((\"right\", \"top\"), True))\n",
    "        plt.show()\n",
    "\n",
    "        # predict\n",
    "        begin = time()\n",
    "        predicted = model.predict(X_test)\n",
    "        end = time()\n",
    "        del X_test\n",
    "        del model\n",
    "        _ = gc_collect()\n",
    "        predict_times.append(end - begin)\n",
    "        print(f\"predicted in {end - begin:.2f} s\")\n",
    "        del end\n",
    "        del begin\n",
    "        _ = gc_collect()\n",
    "        y_pred = (\n",
    "            predicted.argmax(axis=1)\n",
    "            if multiclass\n",
    "            else pd.Series(np.squeeze(predicted)).swifter.apply(round)\n",
    "        ).astype(np.int8)\n",
    "        store_prediction(\n",
    "            where=predictions_,\n",
    "            prediction=Prediction(\n",
    "                model=model_base,\n",
    "                case=case,\n",
    "                fold=fold_num,\n",
    "                y_pred=y_pred,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # score\n",
    "        score_params = {\n",
    "            \"y_true\": y_test,\n",
    "            \"y_pred\": y_pred,\n",
    "        }\n",
    "        balanced_accuracy_adjusted.append(\n",
    "            balanced_accuracy_score(**score_params, adjusted=True)\n",
    "        )\n",
    "        balanced_accuracy.append(balanced_accuracy_score(**score_params))\n",
    "        f1.append(f1_score(**score_params, average=\"weighted\"))\n",
    "        geometric_mean.append(geometric_mean_score(**score_params))\n",
    "        del y_pred\n",
    "        score_params = {\n",
    "            \"y_true\": y_test,\n",
    "            \"y_score\": predicted,\n",
    "            \"average\": \"weighted\",\n",
    "            \"multi_class\": \"ovo\",\n",
    "        }\n",
    "        roc_auc.append(roc_auc_score(**score_params))\n",
    "        del score_params\n",
    "        del predicted\n",
    "        del y_test\n",
    "        _ = gc_collect()\n",
    "\n",
    "        if track_fold_time:\n",
    "            print(\n",
    "                f\"{model_base} fold {fold_num + 1} completed in {time() - fold_begin:.2f} s\"\n",
    "            )\n",
    "            del fold_begin\n",
    "\n",
    "        fold_num += 1\n",
    "        _ = gc_collect()\n",
    "\n",
    "    return AvgScores(\n",
    "        *[\n",
    "            np.mean(_)\n",
    "            for _ in (\n",
    "                balanced_accuracy,\n",
    "                balanced_accuracy_adjusted,\n",
    "                f1,\n",
    "                geometric_mean,\n",
    "                roc_auc,\n",
    "                fit_times,\n",
    "                predict_times,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"negative\"\n",
    "model = \"keras_MLP_14\"\n",
    "\n",
    "eval_begin = time()\n",
    "\n",
    "print(f\"evaluating {model}...\")\n",
    "_ = gc_collect()\n",
    "store_result(\n",
    "    where=results_,\n",
    "    case=case,\n",
    "    model_name=model,\n",
    "    avg_scores=evaluate_model(model, case),\n",
    ")\n",
    "print(f\"stored {model} for {case} classification in {time() - eval_begin:.2f} s\")\n",
    "\n",
    "del case\n",
    "del model\n",
    "del eval_begin\n",
    "_ = gc_collect()\n",
    "\n",
    "create_results_df(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"neutral\"\n",
    "model = \"keras_MLP_14\"\n",
    "\n",
    "eval_begin = time()\n",
    "\n",
    "print(f\"evaluating {model}...\")\n",
    "_ = gc_collect()\n",
    "store_result(\n",
    "    where=results_,\n",
    "    case=case,\n",
    "    model_name=model,\n",
    "    avg_scores=evaluate_model(model, case),\n",
    ")\n",
    "print(f\"stored {model} for {case} classification in {time() - eval_begin:.2f} s\")\n",
    "\n",
    "del case\n",
    "del model\n",
    "del eval_begin\n",
    "_ = gc_collect()\n",
    "\n",
    "create_results_df(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"ternary\"\n",
    "model = \"keras_MLP_14\"\n",
    "\n",
    "eval_begin = time()\n",
    "\n",
    "print(f\"evaluating {model}...\")\n",
    "_ = gc_collect()\n",
    "store_result(\n",
    "    where=results_,\n",
    "    case=case,\n",
    "    model_name=model,\n",
    "    avg_scores=evaluate_model(model, case),\n",
    ")\n",
    "print(f\"stored {model} for {case} classification in {time() - eval_begin:.2f} s\")\n",
    "\n",
    "del case\n",
    "del model\n",
    "del eval_begin\n",
    "_ = gc_collect()\n",
    "\n",
    "create_results_df(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "predictions_df = pd.DataFrame(predictions_)[[\"model\", \"case\", \"fold\", \"y_pred\"]]\n",
    "predictions_df.case = predictions_df.case.astype(\"category\")\n",
    "predictions_df.fold = predictions_df.fold.astype(np.uint8)\n",
    "predictions_df.y_pred = predictions_df.y_pred.swifter.apply(np.int8)\n",
    "predictions_df.to_feather(f\"{DATA_OUT_FOLDER}/keras_MLP_14_predictions_CV5.feather\")\n",
    "del predictions_df\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = create_results_df(results_)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_y_pred(label: int) -> np.ndarray:\n",
    "    \"\"\"Prepares ternary labels for AUROC scoring\"\"\"\n",
    "    _ = [0, 0, 0]\n",
    "    _[label] = 1\n",
    "    return np.asarray(_, dtype=np.int8)\n",
    "\n",
    "\n",
    "model_name = \"keras_MLP_14+keras_MLP_14\"\n",
    "pred_df = pd.DataFrame(predictions_).query(f\"case != 'ternary'\")[\n",
    "    [\"case\", \"fold\", \"y_pred\"]\n",
    "]\n",
    "balanced_accuracy, balanced_accuracy_adjusted, f1, geometric_mean, roc_auc = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for fold_num in np.unique(pred_df.fold):\n",
    "    fold_df = pred_df.query(f\"fold == {fold_num}\").drop(columns=\"fold\")\n",
    "    query_fold = lambda case: fold_df.query(f\"case == '{case}'\").y_pred.item()\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"neutral_prediction\": query_fold(\"neutral\"),\n",
    "            \"negative_prediction\": query_fold(\"negative\"),\n",
    "        }\n",
    "    )\n",
    "    del fold_df\n",
    "    del query_fold\n",
    "    _ = gc_collect()\n",
    "\n",
    "    # \"predict\" 0: negative, 1: neutral, 2: positive\n",
    "    df[\"hybrid_prediction\"] = df.loc[:, \"neutral_prediction\"]\n",
    "    df.loc[df[\"hybrid_prediction\"] == 0, \"hybrid_prediction\"] = (-1) * df.loc[\n",
    "        df[\"hybrid_prediction\"] == 0, \"negative_prediction\"\n",
    "    ] + 2\n",
    "    y_pred: pd.Series = df.hybrid_prediction.astype(np.int8)\n",
    "    del df\n",
    "    _ = gc_collect()\n",
    "\n",
    "    # record y_pred\n",
    "    store_prediction(\n",
    "        where=predictions_,\n",
    "        prediction=Prediction(\n",
    "            model=model_name,\n",
    "            case=\"ternary_negneu\",\n",
    "            fold=fold_num,\n",
    "            y_pred=y_pred,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # score\n",
    "    score_params = {\n",
    "        \"y_true\": np.squeeze(\n",
    "            pd.read_feather(\n",
    "                f\"{FRILL_FEATHERS_FOLDER}/cv_{fold_num}/y_test_ter.feather\"\n",
    "            ).values\n",
    "        ),\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "    balanced_accuracy.append(balanced_accuracy_score(**score_params))\n",
    "    balanced_accuracy_adjusted.append(\n",
    "        balanced_accuracy_score(**score_params, adjusted=True)\n",
    "    )\n",
    "    f1.append(f1_score(**score_params, average=\"weighted\"))\n",
    "    geometric_mean.append(geometric_mean_score(**score_params))\n",
    "    score_params = {\n",
    "        \"y_true\": score_params[\"y_true\"],\n",
    "        \"y_score\": np.stack(y_pred.swifter.apply(prep_y_pred).values),\n",
    "        \"average\": \"weighted\",\n",
    "        \"multi_class\": \"ovo\",\n",
    "    }\n",
    "    roc_auc.append(roc_auc_score(**score_params))\n",
    "    del score_params\n",
    "    del y_pred\n",
    "    _ = gc_collect()\n",
    "del pred_df\n",
    "_ = gc_collect()\n",
    "\n",
    "# helper functions\n",
    "get_time = lambda case, fit_predict: results_df.query(\n",
    "    f\"case == '{case}' & model_name == 'keras_MLP_14'\"\n",
    ")[f\"{fit_predict}_time\"].item()\n",
    "sum_times = lambda time_metric: get_time(\"negative\", time_metric) + get_time(\n",
    "    \"neutral\", time_metric\n",
    ")\n",
    "\n",
    "store_result(\n",
    "    where=results_,\n",
    "    case=\"ternary_negneu\",\n",
    "    model_name=model_name,\n",
    "    avg_scores=AvgScores(\n",
    "        np.mean(balanced_accuracy),\n",
    "        np.mean(balanced_accuracy_adjusted),\n",
    "        np.mean(f1),\n",
    "        np.mean(geometric_mean),\n",
    "        np.mean(roc_auc),\n",
    "        sum_times(\"fit\"),\n",
    "        sum_times(\"predict\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"stored hybrid ternary classification results for {model_name}\")\n",
    "\n",
    "del balanced_accuracy\n",
    "del balanced_accuracy_adjusted\n",
    "del f1\n",
    "del geometric_mean\n",
    "del roc_auc\n",
    "del model_name\n",
    "del get_time\n",
    "del sum_times\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = create_results_df(results_)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table of results\n",
    "results_df.to_csv(f\"{DATA_OUT_FOLDER}/initial_MLP_results_CV5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "hopefully good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Time elapsed since notebook_begin_time: {time() - notebook_begin_time} s\")\n",
    "_ = gc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1781be99c106060f3abc0c9b91d3d379f24672894e2158d4b74304109955878"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
